{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af0a093d-7c3f-4880-a387-e312e39fbcb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b424dc3-6e4f-49ae-90e3-f58f85d3548e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PII Data Security\n",
    "\n",
    "In this demo you will learn how to:\n",
    "\n",
    "* How to handle PII Data Security with **Pseudonymization and Anonymization**\n",
    "\n",
    "Further, you will also learn:\n",
    "* Generate and trigger a Lakeflow Spark Declarative Pipeline that manages both processes\n",
    "* Explore the resultant DAG\n",
    "* Land a new batch of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebdd6421-7365-4c70-895d-10c7dcb47d91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "676f68b5-4b41-4c3f-af7a-1a65aab1bbfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to your unique catalog name and the schema to your specific schema name shown below using the `USE` statements.\n",
    "<br></br>\n",
    "\n",
    "\n",
    "```\n",
    "USE CATALOG your-catalog;\n",
    "USE SCHEMA your-catalog.pii_data;\n",
    "```\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df7526a2-019b-4578-9c21-d03e15830187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c47fcf7-d350-462c-bfe7-8e6038461949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the code below to view your current default catalog and schema. Confirm that they have the same name as the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a85f1c7-3291-434c-915d-b674da8fabb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2d3b5d9-87dc-48ed-9ec3-e8537692dea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## B. Generate and Trigger the Lakeflow Spark Declarative Pipeline\n",
    "Run the cell below to auto-generate your Lakeflow Spark Declarative Pipeline using the provided configuration values.\n",
    "\n",
    "After creation, the pipeline will run. The initial run will take a few minutes while a cluster is provisioned.\n",
    "\n",
    "**NOTE:** The `DeclarativePipelineCreator` class is a custom class we use to setup the Lakeflow Spark Declarative Pipeline. The class is built using the Databricks SDK and RESTAPI calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adabdda3-588b-4a25-98b9-e5fee18a2faa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "demo_pipeline = DeclarativePipelineCreator(\n",
    "    pipeline_name=f\"1.2_PII_Data_Security_{DA.catalog_name}\", \n",
    "    catalog_name=DA.catalog_name,\n",
    "    schema_name=\"pii_data\",\n",
    "    root_path_folder_name='Pipeline',\n",
    "    source_folder_names=[\n",
    "        'DP 1.2.1 - Pseudonymized PII Lookup Table',\n",
    "        'DP 1.2.2 - Anonymized Users Age'\n",
    "    ],\n",
    "    configuration={\n",
    "        'user_reg_source':f'/Volumes/{DA.catalog_name}/pii_data/pii/stream_source/user_reg',\n",
    "        'daily_user_events_source':f\"/Volumes/{DA.catalog_name}/pii_data/pii/stream_source/daily\",\n",
    "        'lookup_catalog': DA.catalog_name\n",
    "        },\n",
    "    serverless=True,\n",
    "    channel='CURRENT',\n",
    "    delete_pipeine_if_exists = True\n",
    ")\n",
    "\n",
    "demo_pipeline.create_pipeline()\n",
    "\n",
    "demo_pipeline.start_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "008ac520-d73b-4218-b3ad-23588379989c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Lakeflow Spark Declarative Pipeline Overview\n",
    "\n",
    "This Lakeflow Spark Declarative Pipeline is based in two notebooks located in the \"Pipeline\" folder:\n",
    "\n",
    "- [DP 1.2.1 - Pseudonymized PII Lookup Table]($./Pipeline/DP 1.2.1 - Pseudonymized PII Lookup Table): Provides an overview of how to ingest and stream **registered_user_data** to apply two **Pseudonymization** techniques such as:\n",
    "  - Hashing\n",
    "  - Tokenization\n",
    "\n",
    "- [DP 1.2.2 - Anonymized Users Age]($./Pipeline/DP 1.2.2 - Anonymized Users Age): Provides an overview of how to ingest and stream **user_events_raw** data into a **users_bronze** and apply **Binning Anonymization** on User's Ages into a materialized view **user_bins**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a191b15-a8f0-4202-8aa2-1a822095c66c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B1. Open the Lakeflow Spark Declarative Pipeline\n",
    "\n",
    "In the left navigation bar, complete the following to open your Lakeflow Spark Declarative Pipeline:\n",
    "\n",
    "1. Right-click on Jobs and Pipelines and select *Open in New Tab*.\n",
    "\n",
    "2. Find and select your pipeline named **1.2_PII_Data_Security_labuser-name**.\n",
    "\n",
    "3. Leave the pipeline page open and continue to the next steps.\n",
    "\n",
    "4. Once the pipeline completes, here is the graphed execution flow:\n",
    "\n",
    "![ddemo01_2_full_pipeline.png](./Includes/images/demo01_2_full_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36d378f7-f767-4f33-86ee-eec215445408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Pseudonymization\n",
    "\n",
    "As a recap:\n",
    "\n",
    "- Switches original data point with pseudonym for later re-identification\n",
    "- Only authorized users will have access to keys/hash/table for re-identification\n",
    "- Protects datasets on record level for machine learning\n",
    "- A pseudonym is still considered to be personal data according to the GDPR\n",
    "Two main pseudonymization methods: hashing and tokenization\n",
    "\n",
    "\n",
    "[DP 1.2.1 - Pseudonymized PII Lookup Table]($./Pipeline/DP 1.2.1 - Pseudonymized PII Lookup Table): Provides an overview of how to ingest and stream **registered_users** to apply two **Pseudonymization** techniques such as:\n",
    "  1. Creates the **registered_users** table from the source JSON files with PII.\n",
    "\n",
    "  1. Hashing: Handled in table **user_lookup_hashed**\n",
    "\n",
    "  1. Tokenization: Handled in tables **registered_users_tokens** and **user_lookup_tokenized**\n",
    "\n",
    "\n",
    "#### Pseudonymization section in DAG\n",
    "\n",
    "![demo01_2_pii_data_security_pseudo_dag.png](./Includes/images/demo01_2_pii_data_security_pseudo_dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b2be7f9-f86d-4200-bffe-e4ccd28b4dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. Preview the registered_users Table\n",
    "\n",
    "The table **registered_users** will be our source for the ingested users, where we'll apply *Pseudonymization* and *Anonymization*. \n",
    "\n",
    "Run the cell and view the original source data. Notice that no data has been anonymized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2612d26-0e90-444d-8c11-947bd6e23d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "    user_id,\n",
    "    device_id,\n",
    "    mac_address\n",
    "FROM registered_users \n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25c2b632-8173-402d-ad54-ab877cca1f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### C2. Option 1 - Hashing\n",
    "\n",
    "Objectives:\n",
    "\n",
    "- Apply SHA or other hashes to all PII.\n",
    "- Add a random string \"salt\" to values before hashing.\n",
    "- Databricks secrets can be leveraged for obfuscating the salt value.\n",
    "- This leads to a slight increase in data size.\n",
    "- Some operations may be less efficient.\n",
    "\n",
    "In our pipeline, we leverage the **registered_users** table and apply hashing to the **user_id** column using a salt value of *BEANS*, creating a column **alt_id** in the **user_lookup_hashed** table.\n",
    "\n",
    "See the cell below for the results and compare both the **user_id** and **alt_id** columns.\n",
    "\n",
    "**NOTE:** The **user_id** column should be removed after processing. It is kept for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fe489f7-a426-4ff8-807d-e50d8a746619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  alt_id,\n",
    "  user_id,\n",
    "  device_id,\n",
    "  mac_address \n",
    "FROM user_lookup_hashed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0515a517-dae2-46c3-b70e-f340da226bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C3. Option 2 - Tokenization\n",
    "\n",
    "**Tokenization** objectives:\n",
    "\n",
    "- Converts all PII to keys.\n",
    "- Values are stored in a secure lookup table.\n",
    "- Slow to write, but fast to read.\n",
    "- De-identified data is stored in fewer bytes.\n",
    "\n",
    "Similar to the previous step, our pipeline leverages the **registered_users** table. This time, the pipeline creates a new table called **registered_users_tokens** to store the relationship between the generated token (using the [uuid function](https://docs.databricks.com/en/sql/language-manual/functions/uuid.html)) and the **user_id** column.\n",
    "\n",
    "See the token column generated for each **user_id** in the **registered_users_tokens** table below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7582cb6d-ef94-4c38-b2b7-c34b2cd7f43c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM registered_users_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c6e7bdf-6001-436b-a1dd-168f57565e2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we can use and leverage the table **registered_users_tokens** and create a new lookup table with tokenized **user_id** column, held in **user_lookup_tokenized** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5e94b0c-6f4f-46ad-b7be-73a1045af3e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  alt_id as Tokenized,\n",
    "  device_id,\n",
    "  mac_address, \n",
    "  registration_timestamp \n",
    "FROM user_lookup_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e86e8e61-9e45-4ae0-8953-77229a35a54a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Anonymization\n",
    "\n",
    "As a recap:\n",
    "\n",
    "- Protects entire dataset (tables, databases or entire data catalogues) mostly for Business Intelligence\n",
    "- Personal data is irreversibly altered in such a way that a data subject can no longer be identified directly or indirectly\n",
    "- Usually a combination of more than one technique used in real-world scenarios\n",
    "- Two main anonymization methods: data suppression and generalization\n",
    "\n",
    "\n",
    "[DP 1.2.2 - Anonymized Users Age]($./Pipeline/DP 1.2.2 - Anonymized Users Age): Provides an overview of how to ingest and stream **user_events_raw** data into a **users_bronze** and apply **Binning Anonymization** on User's Ages into a materialized view **user_age_bins**.\n",
    "\n",
    "#### Anonymization section in DAG\n",
    "\n",
    "![demo01_2_anonymization_dag.png](./Includes/images/demo01_2_anonymization_dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7000cd6-232f-40b4-855b-fc8cb9e73463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D1. Explore the Date Lookup and User Events Raw tables\n",
    "\n",
    "- The **date_lookup** table is used for the **date** and **week_part** association. It is joined with the **user_events_raw** data to identify which **week_part** the **Date of Birth (DOB)** belongs to. \n",
    "  - For example: (date) 2020-07-02 = (week_part) 2020-27."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b2f483a-adad-4d11-a9b2-4208f568685a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM date_lookup\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6793b848-ced5-4d79-bfbf-201ddbe85d89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- The **user_events_raw** table represents the ingested user event data in JSON format, which is later unpacked and filtered to retrieve only user information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a903400f-7b72-43ee-9101-1b3a26211e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  string(key), \n",
    "  string(value)\n",
    "FROM user_events_raw\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a98a9da4-2ac0-404f-b681-12a9d07d40e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D2. Users Bronze\n",
    "\n",
    "The table **users_bronze** is our focus and will be our source for the ingested user information, where we'll apply **Binning Anonymization** to the **Date of Birth (DOB)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19483c66-baa1-47d5-92d1-cde416f16aca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  user_id,\n",
    "  dob,\n",
    "  gender,\n",
    "  city,\n",
    "  state \n",
    "FROM users_bronze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a352ed97-eb91-4cb5-b1f6-8fe97bbe5d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D3. User Age Bins\n",
    "\n",
    "The table **user_age_bins** shows the results of the **binning anonymization** applied, check **age** column and the range provide for each user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0265c066-60c8-4f79-a35e-0e0d1994ac87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM user_age_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d9dd0e7-105f-405b-844c-cd63c603315d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Land New Data and Trigger the Pipeline\n",
    "\n",
    "Run the cell below to land more data in the source directory, then navigate to the Pipelines UI and manually trigger a pipeline update.\n",
    "\n",
    "As we continue through the course, you can return to this notebook and use the method provided below to land new data. Running this entire notebook again will delete the underlying data files for both the source data and your Lakeflow Spark Declarative Pipeline and enable you to start over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3cbb894-6d53-40d2-8ee0-6e4aacd1e1c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Load files into (your catalog -> pii_data -> volumes -> pii -> stream_source -> daily)\n",
    "DA.load(copy_from=DA.paths.stream_source.daily_working_dir,\n",
    "        copy_to=DA.paths.stream_source.daily,\n",
    "        n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21abf3f5-9a23-4ce3-97c8-bd0c7dab59fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "DP 1.2 - PII Data Security",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
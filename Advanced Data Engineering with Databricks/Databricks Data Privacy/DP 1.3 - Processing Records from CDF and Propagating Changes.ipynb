{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbefb9b3-fdc6-44a0-8b5e-e78883364330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84c43e85-f6d1-407f-93d7-72f7eb120797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Processing Records from CDF and Propagating Changes\n",
    "\n",
    "In this notebook, we'll demonstrate how you can easily propagate changes (inserts, updates and deletes) through a Lakehouse with Delta Lake [Change Data Feed (CDF)](https://docs.databricks.com/en/delta/delta-change-data-feed.html) via Stream or queried by its specific version.\n",
    "\n",
    "For this demo, we'll work with a slightly different dataset representing patient information for medical records. Descriptions of the data at various stages follow.\n",
    "\n",
    "#### Raw Files\n",
    "We will be loading JSON files into a Delta table.\n",
    "\n",
    "#### Bronze Table (bronze_users)\n",
    "Here we store all records as consumed. A row represents:\n",
    "1. A new patient providing data for the first time\n",
    "1. An existing patient confirming that their information is still correct\n",
    "1. An existing patient updating some of their information\n",
    "\n",
    "The type of action a row represents is not captured.\n",
    "\n",
    "#### Silver Table\n",
    "This is the validated view of our data. Each patient will appear only once in this table. An upsert statement will be used to identify rows that have changed.\n",
    "\n",
    "\n",
    "#### Gold Table\n",
    "For this example, we'll create a simple gold table leveraging the silver table.\n",
    "\n",
    "\n",
    "By the end of this lesson, students will be able to:\n",
    "- Enable Change Data Feed on a cluster or for a particular table\n",
    "- Describe how changes (insert, update and delete) are recorded\n",
    "- Read CDF output with Spark SQL or PySpark\n",
    "- Leverage the `change_table` function for tracking changes and `_change_type` column to get specific actions in your data. \n",
    "- Retrieve the latest history version from a table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc3916b6-a020-4414-b470-55c10da0acd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40427941-8173-418f-a5cb-4527932dc74c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to your unique catalog name and the schema to your specific schema name shown below using the `USE` statements.\n",
    "<br></br>\n",
    "\n",
    "\n",
    "```\n",
    "USE CATALOG your-catalog;\n",
    "USE SCHEMA your-catalog.pii_data;\n",
    "```\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80bef5c5-57ba-4e60-b5f8-10aab31ee7f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10eb7728-9b06-4b61-8c7d-53bd57b005a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A1. Catalog and Schema\n",
    "\n",
    "Run the code below to view your current default catalog and schema. Ensure that their names match the ones from the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8ac6ff2-de11-45a2-80bc-5d5c941b4e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fefba59f-7bd8-4314-8045-74dd5fb6d127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A2. Source Data Volume\n",
    "\n",
    "Run the code below to view the volume path where the data will be stored for streaming. You can also check the contents in the Catalog Explorer, located in the left navigation pane. Currently no files are loaded to the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f09742ef-67d9-4352-9bc9-0feafb2db89a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Source Data Volume:{DA.paths.cdc_stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8f49d6b-6cee-476b-b1a6-3c022ac1417a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A3. Structure Streaming Checkpoint folder\n",
    "\n",
    "The [checkpoint](https://docs.databricks.com/en/structured-streaming/checkpoints.html) location tracks information that identifies the query, including state data and processed records. When you delete files in a checkpoint directory or switch to a new checkpoint location, the next run of the query starts fresh.\n",
    "\n",
    "Run the code below to view the checkpoint path to use in our structured streaming process. You can also check this in the Catalog Explorer.Currently no files are loaded to the **_checkpoint** folder in the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3de91cd2-70e2-49bb-8c9b-33370e87e46b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Checkpoint Volume:{DA.paths.checkpoints}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97d4497e-6d39-48e3-ba85-fafb0d54f421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Create a Bronze Users Table and Ingest Data with Auto Loader\n",
    "\n",
    "In this section, we'll use Auto Loader to ingest data as it arrives.\n",
    "\n",
    "The steps below include:\n",
    "* Declaring the target table\n",
    "* Creating and starting the stream\n",
    "* Loading data into the source directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c931201b-c0f2-4693-a081-8a108ff4ad31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### B1. Create the bronze_users table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce5f5ffe-358e-49b6-a4ae-0c6c7c25bbff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop Table if exists\n",
    "DROP TABLE IF EXISTS bronze_users;  \n",
    "\n",
    "-- Create bronze_users table\n",
    "CREATE TABLE IF NOT EXISTS bronze_users ( \n",
    "  mrn BIGINT, \n",
    "  dob DATE, \n",
    "  sex STRING, \n",
    "  gender STRING, \n",
    "  first_name STRING, \n",
    "  last_name STRING, \n",
    "  street_address STRING, \n",
    "  zip BIGINT, city STRING, \n",
    "  state STRING, \n",
    "  updated timestamp\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a55cef4a-14c1-4450-8998-1e06ca6fcb8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B2. Create, Start, and Load Stream Data into the Bronze Table\n",
    "\n",
    "The cell below completes the following:\n",
    "- defines a schema for the stream\n",
    "- creates a stream into the **bronze_users** table that runs every **3 seconds**\n",
    "- loads the first batch of JSON files to your *../pii_data/cdf_demo/stream_source/cdc* volume.\n",
    "\n",
    "**Note**: The stream will continue running every 3 seconds until it is terminated later in the notebook. Wait for the stream to finish reading the file before continuing to the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd6a0593-a6da-4a9f-a4c9-66149e6ff434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the incoming data\n",
    "schema = \"\"\"\n",
    "  mrn BIGINT, \n",
    "  dob DATE, \n",
    "  sex STRING, \n",
    "  gender STRING, \n",
    "  first_name STRING, \n",
    "  last_name STRING, \n",
    "  street_address STRING, \n",
    "  zip BIGINT, \n",
    "  city STRING, \n",
    "  state STRING, \n",
    "  updated TIMESTAMP\n",
    "\"\"\"\n",
    "\n",
    "# Read the streaming data from the specified path with the defined schema into the Bronze Table\n",
    "from pyspark.sql.functions import col, expr,date_add\n",
    "\n",
    "bronze_users_stream = (\n",
    "      spark\n",
    "      .readStream\n",
    "        .format(\"cloudFiles\")                 # Specify the format as cloudFiles for auto loader\n",
    "        .option(\"cloudFiles.format\", \"json\")  # Specify the file format as JSON\n",
    "        .schema(schema)                       # Apply the defined schema to the incoming data\n",
    "        .load(DA.paths.cdc_stream)            # Load the data from the specified path\n",
    "      .writeStream\n",
    "        .format(\"delta\")                      # Write the stream data in Delta format\n",
    "        .outputMode(\"append\")                 # Append new records to the table\n",
    "        .trigger(processingTime='3 seconds')\n",
    "        .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/bronze\")  # Specify the checkpoint location\n",
    "        .table(\"bronze_users\"))               # Write the stream data to the Bronze table\n",
    "\n",
    "\n",
    "# Load a file into the volume: DA.paths.cdc_stream\n",
    "DA.load(copy_from=DA.paths.stream_source.cdf_demo, \n",
    "        copy_to=DA.paths.cdc_stream, \n",
    "        n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa9c62a9-6f95-45a2-ae6c-5f4b7a3f19b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B3. Validate a File has been placed in your volume\n",
    "\n",
    "Wait for the stream to initialize and process the file that has been placed to be ingested into the **bronze_users** table. Run the cell below to get the exact names of your volumes. \n",
    "\n",
    "In the catalog explorer check your source files and checkpoint volumes and confirm files have been added. You should see the following:\n",
    "- In the  **cdc** folder a file named *batch01.json*\n",
    "- In the **_checkpoints** a folder named **bronze** corresponding to the **bronze_users** table.\n",
    "\n",
    "**Example**\n",
    "\n",
    "![Source and Checkpoint Folders](./Includes/images/stream_folders.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "840a84c3-a2fc-4a6d-9109-bcc49586ede1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Source Data Volume:{DA.paths.cdc_stream}\")\n",
    "print(f\"Checkpoint Volume:{DA.paths.checkpoints}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9148cab-2d44-4618-94c5-859c60ff1619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B4. Validate Ingested Data in the **bronze_users** Table\n",
    "\n",
    "Query the **bronze_users** table and confirm that *829* users have been loaded into the table from the first batch of JSON files (one file).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc39ead-d58b-4a9a-9196-5d38c1862d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM bronze_users;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cef8db86-c663-4698-ac05-7bdc507cd81e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## C. Create a **silver_users** Target Table and Load It with Our Production Data\n",
    "\n",
    "Our **silver_users** table will be loaded with production data from our users to serve as a baseline. Here, we use `DEEP CLONE` to move read-only data from PROD to our environment, where we have full write/delete access.\n",
    "\n",
    "In this case, our production data is stored as a Delta table in the following location: `Volumes/dbacademy_gym_data/v01/pii/silver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee0aa54c-1243-4ea5-b730-71339bf0cfc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS silver_users;\n",
    "\n",
    "CREATE OR REPLACE TABLE silver_users\n",
    "DEEP CLONE delta.`/Volumes/dbacademy_gym_data/v01/pii/silver/`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f24f1bd8-06d6-4aab-8200-cbdc7009c10f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. Enable Change Data Feed (CDF)\n",
    "\n",
    "You can enable CDF on both new and existing tables.\n",
    "\n",
    "To globally enable CDF on every new table, use the following syntax:\n",
    "\n",
    "```spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", True)```\n",
    "\n",
    "Tables that were not created with CDF enabled will not have it turned on by default, but they can be altered to capture changes with the `ALTER TABLE` statement.\n",
    "\n",
    "For more information, see the [Enable Change Data Feed](https://docs.databricks.com/en/delta/delta-change-data-feed.html#enable-change-data-feed) documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "275c0b21-5997-48de-859c-85f5748294a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE silver_users \n",
    "SET TBLPROPERTIES (delta.enableChangeDataFeed = true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "708137d0-3a8e-4f07-9e6e-34ec0ba7888d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Check if CDF is Enabled\n",
    "\n",
    "Use the `DESCRIBE TABLE EXTENDED` command on the silver table to check if CDF is enabled. In the output, look at the last row under **Table Properties** and confirm that CDF is set with the `[delta.enableChangeDataFeed=true]` property.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc70306d-a0e4-4b9f-9733-16e811ce0f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE EXTENDED silver_users;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28360fdb-8fa3-4713-ab59-ee63a92f49dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Query the **silver_users** table and confirm that it contains *3,132* rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "854bf1ad-9690-4b00-bb8b-8910a6ae1f2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM silver_users;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bfefd72-3560-4694-aee3-79e9fa6b92f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Upsert Data from Bronze to Silver Users\n",
    "\n",
    "In this section, we'll stream data from the **bronze_users** table to the **silver_users** table.\n",
    "\n",
    "We will create the `upsert_to_delta` function to handle the streaming `MERGE INTO` operation for the **silver_users** table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a80e0d48-2bcc-4791-8ba6-fdcb66e733ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D1. `upsert_to_delta` Function\n",
    "\n",
    "Here, we create the upsert logic for the **silver_users** table using a streaming read from the **bronze_users** table.\n",
    "\n",
    "The function below performs the following steps:\n",
    "- The `microBatchDF.createOrReplaceTempView(\"updates\")` line takes the incoming micro-batch DataFrame (`microBatchDF`) and registers it as a temporary SQL view named **updates**.\n",
    "- The `MERGE INTO` statement combines `UPDATE` and `INSERT` actions into one operation using our unique identifier **mrn** as the MERGE condition. This statement uses the incoming micro-batch data view **updates** as the source to update the target **silver_users**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6f123b0-7e83-42d6-bfbf-ec9dfe225a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "    # Create or replace a temporary view for the micro-batch DataFrame\n",
    "    microBatchDF.createOrReplaceTempView(\"updates\")\n",
    "    \n",
    "    # Perform a MERGE operation to upsert data into the silver table\n",
    "    # The MERGE statement matches records in the 'silver' table with records in the 'updates' view based on the 'mrn' field\n",
    "    # If a match is found and any of the specified fields are different, the existing record in 'silver' is updated with the new values from 'updates'\n",
    "    # If no match is found, a new record is inserted into 'silver' with the values from 'updates'\n",
    "    microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "        MERGE INTO silver_users s\n",
    "        USING updates u\n",
    "        ON s.mrn = u.mrn\n",
    "        WHEN MATCHED AND s.dob <> u.dob OR\n",
    "                         s.sex <> u.sex OR\n",
    "                         s.gender <> u.gender OR\n",
    "                         s.first_name <> u.first_name OR\n",
    "                         s.last_name <> u.last_name OR\n",
    "                         s.street_address <> u.street_address OR\n",
    "                         s.zip <> u.zip OR\n",
    "                         s.city <> u.city OR\n",
    "                         s.state <> u.state OR\n",
    "                         s.updated <> u.updated\n",
    "            THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED\n",
    "            THEN INSERT *\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eacc7f5-86e4-45a3-9062-995195272b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D2. Stream user's data from Bronze into Silver tables\n",
    "\n",
    "Now lets create and start the stream from **bronze_users** into **silver_users**. The `foreachBatch` method uses the `upsert_to_delta` function from above to upsert from a streaming query.\n",
    "\n",
    "Check the [Upsert from streaming queries using foreachBatch](https://docs.databricks.com/en/structured-streaming/delta-lake.html#merge-in-streaming) documentation for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd6cfbc9-cb1e-4882-98a9-efe3af1e8494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_users_stream = (\n",
    "              spark\n",
    "              .readStream\n",
    "                .table(\"bronze_users\")\n",
    "              .writeStream\n",
    "                .foreachBatch(upsert_to_delta)  # Upsert micro-batch data into the silver table\n",
    "                .trigger(processingTime='3 seconds')  # Trigger the stream processing every 3 seconds\n",
    "                .start()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f00ff146-2893-4139-a5b3-9976035e475e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D3. Check the History of the Silver Users Table\n",
    "\n",
    "Wait for the stream to initialize and process.\n",
    "\n",
    "Once complete, let's check the changes applied to our **silver_users** table.\n",
    "\n",
    "Running the cell below will show the history for this table, and there should be three log entries:\n",
    "- Version 0: The initial clone\n",
    "- Version 1: Setting table properties to enable CDF on the table\n",
    "- Version 2: The MERGE stream from the **bronze_users** table.\n",
    "\n",
    "**NOTE:** If the results show only two versions, please wait for the stream to complete and then rerun the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f2e46bf-e14b-4359-a6d7-ad3f0370bb99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY silver_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faf46f30-91b0-4a51-b3af-f12835b8f4c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Access and Read the Change Data Feed (CDF)\n",
    "\n",
    "The [Change Data Feed (CDF)](https://docs.databricks.com/en/delta/delta-change-data-feed.html) feature allows tracking of row-level changes between versions of a Delta table. When enabled, it records change events for all data written to the table, including metadata that indicates whether a row was inserted, deleted, or updated.\n",
    "\n",
    "To capture the recorded CDC data in a stream, we'll add two options:\n",
    "- **`readChangeData` = True**\n",
    "- **`startingVersion` = 2** (This refers to the version history; alternatively, you can use **`startingTimestamp`**)\n",
    "\n",
    "## CDF Schema in Results\n",
    "When reading from the change data feed, the schema includes the following metadata columns:\n",
    "\n",
    "| Column name        | Type      | Description                                    |\n",
    "|--------------------|-----------|------------------------------------------------|\n",
    "| _change_type       | String    | The type of change event: `insert`, `delete`, `update_preimage`, `update_postimage`. |\n",
    "| _commit_version    | Long      | The Delta log or table version containing the change |\n",
    "| _commit_timestamp  | Timestamp | The timestamp when the commit was created      |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eab17189-6620-4132-97e8-17fa2028198e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this section, we’ll display all changes to patients in the **silver_users** table starting at version *2*.\n",
    "\n",
    "Run the cell below and view the results. Notice that the results show 1,530 records have been modified.\n",
    "\n",
    "Then, scroll to the right of the table. You will notice the new columns: **_change_type**, **_commit_version**, and **_commit_timestamp**.\n",
    "\n",
    "**NOTE:** Users with changes will have two records: one corresponding to *update_preimage* and one corresponding to *update_postimage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a85e0fc-b6ed-4756-abc6-a240dc0a43c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cdf_df = (spark.read\n",
    "               .format(\"delta\")\n",
    "               .option(\"readChangeData\", True)   # Read the change data\n",
    "               .option(\"startingVersion\", 2)     # Since we want to start reading changes from version 2 (when we merged the data)\n",
    "               .table(\"silver_users\"))\n",
    "\n",
    "## Display the changed data\n",
    "display(cdf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee0798d3-27cc-44d7-8126-61f00cc1dda1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E1. Land New Data to Stream\n",
    "\n",
    "As a recap, we currently have two active streams:\n",
    "\n",
    "1. The first stream ingests data from our source folder (raw JSON files) into the **bronze_users** table.\n",
    "2. The second stream syncs data via `MERGE INTO` from the **bronze_users** table to the **silver_users** table.\n",
    "\n",
    "Run the cell below to land another JSON file in our source directory and stream additional data into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f0dbaf6-197f-497f-ba6e-ec94b15cdbdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load files into the volume: DA.paths.cdc_stream\n",
    "DA.load(copy_from=DA.paths.stream_source.cdf_demo, \n",
    "        copy_to=DA.paths.cdc_stream, \n",
    "        n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81442c6e-a8f3-4e15-9c0d-48a3918c05b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E2. Check again silver_users Table History\n",
    "\n",
    "#### NOTE: Wait a few seconds for the stream to process the newly added JSON file and upsert data into the **silver_users** table.\n",
    "\n",
    "Our objective is to validate the captured CDC changes for **`_commit_version` number 3** (you can change the sort order of the **`_commit_version`** column in the display above to see this).\n",
    "\n",
    "Run the cell below to view the history of the **silver_users** table after the new file was added to the volume. \n",
    "\n",
    "After running the cell, view the results and confirm that version *3* is now available in the history with a *MERGE* operation. This should correspond to the last cell where new files were loaded.\n",
    "\n",
    "**NOTE:** If three rows are returned in the results, please wait a few more seconds for the stream to complete, then rerun the cell. The history should display 4 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cde6bcc-cefc-4cd2-b4be-85a9906ee082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY silver_users;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5508ab8-be80-4eb8-a038-c9b942f29ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. Table Changes Function\n",
    "\n",
    "To pick up the recorded CDC data for a specific range of table's history versions there is the [table_changes](https://docs.databricks.com/en/sql/language-manual/functions/table_changes.html) function is part of the Delta Lake Change Data Feed (CDF) feature, which tracks row-level changes between versions of a Delta table. It returns a log of changes to a Delta Lake table with Change Data Feed enabled, including inserts, updates, and deletes.\n",
    "\n",
    "Use the following syntax: `table_changes(table_str, start [, end])`\n",
    "- _table_str_:  The table name\n",
    "- _start_: starting history version or timestamp\n",
    "- _end_: optional ending version or timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "380a1a18-adc0-4104-8ff3-b188385d9e84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### F1. Get the Latest Version of the Table's History\n",
    "\n",
    "The cell below will capture the most recent version of the **silver_users** table and store it in an SQL variable.\n",
    "\n",
    "Confirm that the variable **latest_version** equals *3*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88811f83-df0e-4a37-b00a-1314e29de98b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop the existing variable if it exists\n",
    "DROP TEMPORARY VARIABLE IF EXISTS latest_version;\n",
    "\n",
    "-- Declare the variable\n",
    "DECLARE VARIABLE latest_version INT;\n",
    "\n",
    "-- Set the variable to the latest version of the table\n",
    "SET VARIABLE latest_version = (\n",
    "  SELECT max(version) AS latest_version\n",
    "  FROM (DESCRIBE HISTORY silver_users)\n",
    ");\n",
    "\n",
    "-- Select the variable\n",
    "SELECT latest_version;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "073d26af-1395-4716-932a-c25b22884525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### F2. Get Operation Metrics for Inserted, Updated, and Deleted Rows\n",
    "\n",
    "The cell below will query the table’s latest history using the **latest_version** variable and display the following operation metrics for the most recent update/insert/delete:\n",
    "- **numTargetRowsInserted**: The number of rows inserted in the latest version (147 inserted).\n",
    "- **numTargetRowsUpdated**: The number of rows updated in the latest version (809 updated).\n",
    "- **numTargetRowsDeleted**: The number of rows deleted in the latest version (0 deleted).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86faa8de-83ed-4521-92d7-69506e8a5c4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  operationMetrics['numTargetRowsInserted'],\n",
    "  operationMetrics['numTargetRowsUpdated'],\n",
    "  operationMetrics['numTargetRowsDeleted']\n",
    "FROM (DESCRIBE HISTORY silver_users)\n",
    "WHERE version = latest_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "763d883c-2dc3-4935-baba-89e834c9c2cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### F3. View Modified Rows (Inserted, Updated or Deleted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9891e31c-9d29-48ad-b44c-ac2033d1e19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The **_change_type** column allows us to easily see how rows were modified.\n",
    "\n",
    "- *insert*: Indicates a new row was added to the table.\n",
    "- *delete*: Indicates a row was removed from the table.\n",
    "- *update_preimage*: Represents the value of a row before it was updated.\n",
    "- *update_postimage*: Represents the value of a row after it was updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a069fa9-381e-4101-9b03-6ad39c894b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### F3.1 View All Rows That Were Inserted (insert)\n",
    "\n",
    "In the cell below, use the `table_changes` function to query most recent update to the **silver_users** table and filter for all rows that were inserted. Use the **_change_type** column to filter for rows with the value *insert*. Run the cell.\n",
    "\n",
    "Notice that the resulting row count matches the one provided in the previous cell: ('numTargetRowsInserted') = *147*. \n",
    "\n",
    "Scroll to the right of the table and view the columns **_change_type**, **_commit_version** and **_commit_timestamp**.\n",
    "\n",
    "\n",
    "With CDF, you can easily inspect every row that was inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0117338-80ef-49cd-ae86-0d880b8e05f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM table_changes(\"silver_users\", latest_version)    -- Query the latest update\n",
    "WHERE _change_type = \"insert\"                         -- View all rows that were inserted\n",
    "ORDER BY _commit_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fffcdec-436e-4459-8371-57d14b93d923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### F3.2 View Updated Row (update_postimage)\n",
    "\n",
    "In the cell below, use the `table_changes` function to query the most recent update to the **silver_users** table and filter for all updated rows. Use the **_change_type** column to filter for rows with the value *update_postimage*. Run the cell.\n",
    "\n",
    "Notice that the resulting row count matches the one provided in the previous cell: ('numTargetRowsUpdated'): *809*. \n",
    "\n",
    "Scroll to the right of the table and view the columns **_change_type**, **_commit_version** and **_commit_timestamp**.\n",
    "\n",
    "With CDF, you can easily inspect every updated row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22a2b181-c9c9-4c8f-a5e5-ee914650ffe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM table_changes(\"silver_users\", latest_version)\n",
    "WHERE _change_type = \"update_postimage\"\n",
    "ORDER BY _commit_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2520c44-4227-4d4c-a401-df55b2515682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### F3.3 View Row Prior to Update (update_preimage)\n",
    "\n",
    "In the cell below, use the `table_changes` function to query the most recent update to the **silver_users** table and filter for all rows prior to the update. Use the **_change_type** column to filter for rows with the value *update_preimage*. Run the cell.\n",
    "\n",
    "Notice that the resulting row count matches the one provided in the previous cell: ('numTargetRowsUpdated'): *809*. \n",
    "\n",
    "Scroll to the right of the table and view the columns **_change_type**, **_commit_version** and **_commit_timestamp**.\n",
    "\n",
    "With CDF, you can easily inspect every row prior to the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "221da8a5-9cf4-41f2-bf27-c5da90f9bd9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM table_changes(\"silver_users\", latest_version)\n",
    "WHERE _change_type = \"update_preimage\"\n",
    "ORDER BY _commit_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcba42c6-6367-41b6-aba9-08d8d7c01a41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### F3.4 View All Deleted Rows (delete)\n",
    "\n",
    "In the cell below, use the `table_changes` function to query the most recent update to the **silver_users** table and filter for all rows that were deleted. Use the **_change_type** column to filter for rows with the value *delete*. Run the cell.\n",
    "\n",
    "Notice that the resulting row count matches the one provided in the previous cell: ('numTargetRowsDeleted'): *0*. \n",
    "\n",
    "With CDF, you can easily inspect every deleted row.\n",
    "\n",
    "**NOTE:** Don't worry! We'll review how to apply deletes and propagate them in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b0b03ae-5ee4-4336-845b-8b7663532559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM table_changes(\"silver_users\", latest_version)\n",
    "WHERE _change_type in (\"delete\")\n",
    "ORDER BY _commit_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ad070c1-d212-4c49-b475-d38871744ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## G. Propagating Deletes\n",
    "\n",
    "While some use cases may require processing deletes alongside updates and inserts, the most important delete requests are those that allow companies to maintain compliance with privacy regulations such as GDPR and CCPA. Most companies have stated SLAs around how long these requests will take to process, but for various reasons, these are often handled in pipelines separate from their core ETL.\n",
    "\n",
    "This section is focused to Propagate Deletes applied in the **silver_users** to propagate into the **gold_users** table.\n",
    "\n",
    "**NOTE:** Please follow all regulatory standards for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a8533d3-b98b-4e73-9b9f-f0af5ff3902d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### G1. Gold Users Table Setup\n",
    "\n",
    "Let's first create the **gold_users** table, our final production table.\n",
    "\n",
    "Run the query below to create the **gold_users** table and populate it with the latest snapshot of the **silver_users** table. After running the query, view the results and confirm that the table contains *3,407* records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "420c0065-82b0-413d-bd1e-01ae9f905c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS gold_users;\n",
    "\n",
    "-- Create the gold_users table from the silver_users table\n",
    "CREATE OR REPLACE TABLE gold_users as\n",
    "SELECT \n",
    "     mrn,\n",
    "     street_address,\n",
    "     zip,\n",
    "     city,\n",
    "     state,\n",
    "     updated\n",
    "FROM silver_users;\n",
    "\n",
    "-- View the gold table\n",
    "SELECT * \n",
    "FROM gold_users;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "900a449c-2105-49c6-83bb-ae2e801f8194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### G2. Processing Right to Be Forgotten Requests\n",
    "\n",
    "While it is possible to process deletes at the same time as appends and updates, the fines around right to be forgotten requests may warrant a separate process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8159d96c-721e-4ade-a07d-ad198a3bd3d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's start by leveraging the **user_delete_requests** table. This table holds the **mrn**, and the **request_date** (as of today) for users to be deleted, as provided by the compliance team.\n",
    "\n",
    "Let's query the table and view the deletion requests. Confirm that the table currently contains 20 requests for deletion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "523d929e-b281-4f57-8ef2-8075e1f594d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM user_delete_requests;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "647bffb2-8eaf-40cc-850c-0c8c9d89f41a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "We’ll create a new read stream from the **user_delete_requests** table to complete the following:\n",
    "- Add a column named **deadline** to indicate a 30-day period for action, with a default status of *requested*.\n",
    "- Add a column named **status** and update it from *requested* to *deleted* once the changes are propagated.\n",
    "\n",
    "\n",
    "Run the query and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a694b8e1-f7d7-40cb-a652-522715dd137b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "requests_df = (spark.readStream\n",
    "                    .table(\"user_delete_requests\")\n",
    "                    .select(\n",
    "                            \"mrn\",\n",
    "                            \"request_date\",\n",
    "                            F.date_add(\"request_date\", 30).alias(\"deadline\"),\n",
    "                            F.lit(\"requested\").alias(\"status\")))\n",
    "\n",
    "\n",
    "display(requests_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11667a3f-7c89-447c-9b65-fea211de6a44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### G3. Adding Commit Messages in History\n",
    "\n",
    "Delta Lake supports arbitrary commit messages, which are recorded in the Delta transaction log and viewable in the table history. This feature can assist with auditing.\n",
    "\n",
    "Setting a global commit message with SQL will ensure it is used for all subsequent operations in the notebook.\n",
    "\n",
    "For more information, refer to the [Enrich Delta Lake tables with custom metadata](https://docs.databricks.com/en/delta/custom-metadata.html#enrich-delta-lake-tables-with-custom-metadata) documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "646add76-1f06-4158-a217-ff7176748389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SET spark.databricks.delta.commitInfo.userMetadata=Deletes committed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82ddb8f6-d620-4218-9024-f7c8404c8952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "With DataFrames, commit messages can also be specified as part of the write options using `option(\"userMetadata\",\"comment\")`.\n",
    "\n",
    "In this section, we will create a new streaming table named **delete_requests** to indicate that we are manually processing these requests in the notebook, rather than using an automated job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "751ebf6f-fe03-4671-8187-f7a59bc4767d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (requests_df\n",
    "         .writeStream\n",
    "            .outputMode(\"append\")        # Append mode to add new rows to the output table\n",
    "            .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/delete_requests\")  # Specify checkpoint location\n",
    "            .option(\"userMetadata\", \"Requests processed interactively\")  # Add user metadata\n",
    "            .trigger(availableNow=True)  # Trigger the query to process all available data now\n",
    "            .table(\"delete_requests\"))   # Write the output to the delete_requests table\n",
    "\n",
    "\n",
    "query.awaitTermination()  # Wait for the streaming query to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1fe33c2-2fdb-40e8-a17d-2cfdfd589d5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "View the history of the **delete_requests** table. Notice that the **operation** column messages clearly indicate *CREATE TABLE* and *STREAMING UPDATE* in the table history.\n",
    "\n",
    "Scroll to the right of the table. Notice the following:\n",
    "- that the **userMetadata** column contains the note metadata note *Requests processed interactively* that we set above for the stream.\n",
    "- the initial *CREATE TABLE* contains the user metadata *Deletes committed* for the creation of the table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c82bdfaf-99c5-499a-bd8d-80b88eccec49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY delete_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa58e412-a6b2-4613-8335-20610b897285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### G4. Processing Delete Requests\n",
    "\n",
    "The **delete_requests** table will be used to track users' requests to be forgotten. \n",
    "\n",
    "It is possible to process delete requests alongside inserts and updates to existing data as part of a normal **`MERGE`** statement.\n",
    "\n",
    "Because PII exists in several places through the current lakehouse, tracking requests and processing them asynchronously may provide better performance for production jobs with low latency SLAs. The approach modeled here also indicates the time at which the delete was requested and the deadline, and provides a field to indicate the current processing status of the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12b66fc6-dc5f-4cd7-8329-a32fc9402aa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Query the new **delete_requests** table and view the results. Notice that the deletion requests are active, with a **status** is *requested*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d65f613d-3cc0-4b8a-845a-9ac25273f8cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM delete_requests;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e28fdea7-69fb-4185-bed9-f4c3e47c7d61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### G5. Check the Records to Delete\n",
    "\n",
    "When working with static data, committing deletes is straightforward. Run the cell below to preview the records to delete from the **silver_users** table. Notice that the **silver_users** table contains all users who have requested deletion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7a5be1b-893b-4480-9a5e-88da15387b1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM silver_users\n",
    "WHERE mrn IN (SELECT mrn FROM delete_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9cf09d0-c91c-4279-a4e6-46535d9851d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### G6. Committing Deletes into Silver Users\n",
    "\n",
    "The following cell deletes records from the **silver_users** table by rewriting all data files containing records affected by the `DELETE` statement. \n",
    "\n",
    "**NOTE:** Recall that with Delta Lake, deleting data will create new data files rather than deleting existing data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcfd9def-24a0-40a8-810f-22d2ed24c520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DELETE FROM silver_users\n",
    "WHERE mrn IN (SELECT mrn FROM delete_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dd97acb-0a6b-41ca-b57f-adaaecc9c6a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Describe the history of the **silver_users** table and confirm *4* versions of the table exists. The latest version contains the *DELETE* value in the **operation** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0b998c6-b341-4799-bdbf-1cf77f33e684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY silver_users;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c6a917f-2008-4720-a03a-99f3b4a26cfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### G7. Collect Deleted Silver Users to Propagate with CDF\n",
    "\n",
    "The code below configures an incremental read of all changes committed to the **silver_users** table starting at version *4*, the delete operation.\n",
    "\n",
    "Run the cell and view the results. Scroll to the right of the table and notice that in this version *20* rows were deleted from the **silver_users** table. You can view the exact rows that were deleted with CDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1562f38d-96c7-4a74-a180-7eaf84378c9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deleteDF = (spark.readStream\n",
    "                 .format(\"delta\")\n",
    "                 .option(\"readChangeFeed\", \"true\")\n",
    "                 .option(\"startingVersion\", 4)     # Start_version 4 where the delete operation occurred\n",
    "                 .table(\"silver_users\"))\n",
    "\n",
    "\n",
    "display(deleteDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "354e98aa-b552-4799-8473-be863115d516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### G8. Function to Propagate Deletes\n",
    "\n",
    "The relationships between our natural keys (**mrn**) are stored in the **silver_users** table. These keys allow us to link a user's data across various pipelines and sources. The Change Data Feed (CDF) from this table will retain all these fields, enabling successful identification of records to be deleted or modified in downstream tables. This approach can be expanded to use hashed values or other relevant keys.\n",
    "\n",
    "The function below demonstrates how to commit deletes to two tables using different keys and syntax. Note that, in this case, the `MERGE INTO` syntax is not necessarily the only method to process deletes to the **gold_users** table. However, this code block demonstrates the basic syntax that could be extended if inserts and updates were processed alongside deletes in the same operation.\n",
    "\n",
    "Assuming successful completion of these two table modifications, an update will be processed back to the **delete_requests** table as well. \n",
    "\n",
    "The code below completes the following for each batch:\n",
    "- Deletes rows in the **gold_users** table that have been requested.\n",
    "- Updates the status of the requested deletes in the **delete_requests** table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5930f3b-fea8-4799-a4b0-9ce4f0747124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_deletes(microBatchDF, batchId):\n",
    "    \n",
    "    (microBatchDF\n",
    "        .createOrReplaceTempView(\"deletes\"))\n",
    "    \n",
    "    microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "        MERGE INTO gold_users u\n",
    "        USING deletes d\n",
    "        ON u.mrn = d.mrn\n",
    "        WHEN MATCHED\n",
    "            THEN DELETE\n",
    "    \"\"\")\n",
    "\n",
    "    \n",
    "    microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "        MERGE INTO delete_requests dr\n",
    "        USING deletes d\n",
    "        ON d.mrn = dr.mrn\n",
    "        WHEN MATCHED\n",
    "          THEN UPDATE SET status = \"deleted\"\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f411424-e0e4-4887-8496-3c9504cdaeb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### G9. Propagate Changes into Gold Users and Delete Requests\n",
    "\n",
    "Recall that this workload is driven by incremental changes to the **silver_users** table (tracked through the Change Data Feed).\n",
    "\n",
    "Executing the following cell will propagate deletes from a single table to multiple tables throughout the lakehouse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69be003c-a2b6-4563-82f0-39a315948d53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (deleteDF.writeStream\n",
    "                 .foreachBatch(process_deletes)\n",
    "                 .outputMode(\"update\")\n",
    "                 .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/deletes\")\n",
    "                 .trigger(availableNow=True)\n",
    "                 .start())\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b50b9dcd-fd22-4058-88b1-4bd0c19be0f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### G10. Review Delete Commits\n",
    "Notice the **status** for the records in the **delete_requests** table are now updated to *deleted*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60e7d165-a66c-4b69-9533-3e34f0861727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM delete_requests;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee565367-08da-4e47-94d7-491fedd7702e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Describe the history of the **gold_users** table.\n",
    "\n",
    "Notice that in the latest version that our commit message will be in the far right column of our history, under the column **userMetadata**.\n",
    "\n",
    "For the **gold_users** table, the **operation** column in the history will indicate a merge because of the chosen syntax, even though only deletes were committed. The number of deleted rows can be reviewed in the **operationMetrics** in the key *numTargetRowsDeleted*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "971cd4e7-fb7f-456b-95cb-18dffe108f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY gold_users;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f341ba22-4d73-47f4-86e2-c673de177e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Count the number of rows in the current **gold_users** table. Confirm that the table now has *3,387* rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d26adde7-b11b-4ae6-ab36-38345c9e00e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*) AS TotalRows\n",
    "FROM gold_users;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de1407cd-057d-48e4-89cf-cf7dd3669d8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### G11. Are Deletes Fully Committed?\n",
    "\n",
    "Not exactly.\n",
    "\n",
    "Due to how Delta Lake's history and CDF features are implemented, deleted values are still present in older versions of the data.\n",
    "\n",
    "Run the query below to count the number of records from version 0 of the **gold_users** table. The results show that the original table has 3,407 rows, which includes the deleted rows.\n",
    "\n",
    "With Delta tables, you can still view the original data in an earlier version of the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1417d06-4430-417b-9c24-67fa45b9f76b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*) AS TotalRows\n",
    "FROM gold_users VERSION AS OF 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e434d1b-207b-42c8-a08f-b4cda0ebf877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For more information check out [GDPR and CCPA compliance with Delta Lake](https://docs.databricks.com/en/security/privacy/gdpr-delta.html#how-delta-lake-simplifies-point-deletes) and the [VACUUM](https://docs.databricks.com/en/sql/language-manual/delta-vacuum.html) statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1e22593-3ce8-455f-83b8-0c506f28bd23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## H. Stop Active Streams\n",
    "Make sure to run the following cell to stop all active streams. Be careful when using streaming. If you do not stop an active stream, the cluster will continuously run. We are done with streaming data in this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16710179-9d2a-421e-9eff-7bb5fbcefabc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for stream in spark.streams.active:\n",
    "    stream.stop()\n",
    "    stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "728e0570-4d45-4427-99d9-77cf7911a34a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "DP 1.3 - Processing Records from CDF and Propagating Changes",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
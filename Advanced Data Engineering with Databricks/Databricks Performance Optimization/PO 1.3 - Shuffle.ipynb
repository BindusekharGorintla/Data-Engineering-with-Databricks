{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2005cbb1-51ae-4067-b3f7-37690beb547b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10148116-1b1c-4ed1-8969-6fd3436680f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Shuffle\n",
    "\n",
    "Shuffle is a Spark mechanism that redistributes data so that it's grouped differently across partitions. This typically involves copying data across executors and machines and, while it's sometimes necessary, it can be a complex and costly operation.\n",
    "\n",
    "In this demo we will see shuffle in action. Run the next cell to set up the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43a65450-15fa-49b1-98f6-8f1f091d5beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default. If you use Serverless, errors will be returned when setting compute runtime properties.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecc35556-a52b-4218-b1de-6fd1d5d6065e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to your unique **labuser** catalog, and the default schema to **default**. All tables will be read from and written to this location.\n",
    "<br></br>\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d35e0856-dd17-471b-92a4-18be24048a66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5853a632-c1c1-4547-af22-17ab687e1d70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's Check the Current Catalog and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0aa9bf9-a7c6-4ba8-88c9-d9ca0ecc9d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "832098e2-73fd-4f53-a71f-9d63072c18f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Disable Caching\n",
    "\n",
    "Run the following cell to set a Spark configuration variable that disables disk caching.\n",
    "\n",
    "Turning disk caching off prevents Databricks from storing cloud storage files after the first query. This makes the effect of the optimizations more apparent by ensuring that files are always pulled from cloud storage for each query.\n",
    "\n",
    "For more information, see [Optimize performance with caching on Databricks](https://docs.databricks.com/en/optimizations/disk-cache.html#optimize-performance-with-caching-on-databricks).\n",
    "\n",
    "**NOTE:** This will not work in Serverless. Please use classic compute to turn off caching. If you're using Serverless, an error will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "577a4a83-0608-4eb2-b58b-408cea9a32ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.databricks.io.cache.enabled', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a7386e3-c242-4f47-a738-5ad8b9cbca29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## C. Creating Tables\n",
    "Create the following tables for the demonstration:\n",
    "- **transactions**\n",
    "- **stores**\n",
    "- **countries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96cae0dd-6ebd-43a4-b1f9-effd38009c1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. Create the transactions table\n",
    "Let's generate the data we will use in this demo. First, we'll synthesize data representing a set of sales transactions and write the data to a table named **transactions**.\n",
    "\n",
    "The table will contain *150,000,000* rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db93bfb1-7592-4cb4-8f60-aa7d31b07e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "## Drop the table if it exists\n",
    "spark.sql('DROP TABLE IF EXISTS transactions')\n",
    "\n",
    "## Create the Spark DataFrame and save as a table\n",
    "(spark\n",
    " .range(0, 150000000, 1, 32)\n",
    " .select(\n",
    "    'id',\n",
    "    round(rand() * 10000, 2).alias('amount'),\n",
    "    (col('id') % 10).alias('country_id'),\n",
    "    (col('id') % 100).alias('store_id')\n",
    " )\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .saveAsTable('transactions')\n",
    ")\n",
    "\n",
    "## Preview the table\n",
    "display(spark.sql('SELECT * FROM transactions LIMIT 10'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "285f34f7-e293-46b6-b501-c290b3787bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### C2. Create the stores table\n",
    "Now let's synthesize data and write it to a table describing points of sale information named **stores**.\n",
    "\n",
    "The table will contain 99 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc5480b8-5d10-430c-9947-c6cc906e09aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Drop the table if it exists\n",
    "spark.sql('DROP TABLE IF EXISTS stores')\n",
    "\n",
    "## Create the Spark DataFrame\n",
    "(spark\n",
    " .range(0, 99)\n",
    " .select(\n",
    "    'id',\n",
    "    round(rand() * 100, 0).alias('employees'),\n",
    "    (col('id') % 10).alias('country_id'),\n",
    "    expr('uuid()').alias('name')\n",
    "    )\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .saveAsTable('stores')\n",
    ")\n",
    "\n",
    "## Display the table\n",
    "display(spark.sql('SELECT * FROM stores LIMIT 10'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c18663a1-5cfc-42f3-8395-4783aedf2209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C3. Create the countries lookup table \n",
    "Now, let's create a lookup table that maps **country_id** from the data tables to the actual country name.\n",
    "\n",
    "The **countries** table will contain 12 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be5e34aa-4bd9-43dd-a855-04274b830d4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Drop the table if it exists\n",
    "spark.sql('DROP TABLE IF EXISTS countries')\n",
    "\n",
    "## Create the data\n",
    "countries = [(0, \"Italy\"),\n",
    "             (1, \"Canada\"),\n",
    "             (2, \"Mexico\"),\n",
    "             (3, \"China\"),\n",
    "             (4, \"Germany\"),\n",
    "             (5, \"UK\"),\n",
    "             (6, \"Japan\"),\n",
    "             (7, \"Korea\"),\n",
    "             (8, \"Australia\"),\n",
    "             (9, \"France\"),\n",
    "             (10, \"Spain\"),\n",
    "             (11, \"USA\")\n",
    "            ]\n",
    "\n",
    "columns = [\"id\", \"name\"]\n",
    "\n",
    "## Create the Spark DataFrame and countries table\n",
    "countries_df = (spark\n",
    "                .createDataFrame(data = countries, schema = columns)\n",
    "                .write\n",
    "                .mode('overwrite')\n",
    "                .saveAsTable(\"countries\")\n",
    "            )\n",
    "\n",
    "\n",
    "## Display the table\n",
    "display(spark.sql('SELECT * FROM countries'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a07701c-1270-4837-aca8-d33eecf2dc8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##D. Joins\n",
    "\n",
    "Now we'll perform a query that induces shuffling by joining three tables together, writing the results to a separate table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6e0e631-d8d1-4521-835b-67068d7b8712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D1. Disabling Broadcast Joins to Demonstrate Shuffle\n",
    "\n",
    "These options automatically determine when a broadcast join should be used based on the size of the smaller DataFrame (or table) in the join.\n",
    "\n",
    "- [spark.sql.autoBroadcastJoinThreshold](https://spark.apache.org/docs/latest/sql-performance-tuning.html#automatically-broadcasting-joins) documentation\n",
    "\n",
    "- [spark.databricks.adaptive.autoBroadcastJoinThreshold](https://spark.apache.org/docs/latest/sql-performance-tuning.html#converting-sort-merge-join-to-broadcast-join) documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3278b11e-06cc-45a9-bbad-428cd4f866c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to view the default values of the broadcast join and the Adaptive Query Execution(AQE) configurations. Notice that:\n",
    "- The default value for a table with a broadcast join is **10,485,760 bytes**.\n",
    "- If AQE is enabled, the value for the broadcast join is **31,457,280 bytes**.\n",
    "- Note that AQE is enabled by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e490323-f800-4741-bf7b-3974633b2165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f'Default value of autoBroadcastJoinThreshold: {spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")}')\n",
    "print(f'Default value of adaptive.autoBroadcastJoinThreshold: {spark.conf.get(\"spark.databricks.adaptive.autoBroadcastJoinThreshold\")}')\n",
    "print(f'View if AQE is enabled: {spark.conf.get(\"spark.sql.adaptive.enabled\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "675ecaad-4f66-47f4-bff4-0bec330e6d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this cell, we're explicitly turning off broadcast joins in order to demonstrate a shuffle and show you how to investigate and improve the query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ecb5cba-e20b-4aed-8cc6-9603bb8314d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Disabling the automatic broadcast join entirely. That is, Spark will never broadcast any dataset for joins, regardless of its size.\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "# Disabling the broadcast join feature under AQE, meaning that even when using adaptive query execution, Spark will not attempt to broadcast any smaller side of a join.\n",
    "spark.conf.set(\"spark.databricks.adaptive.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "\n",
    "## View the new values\n",
    "print(f'Current value of autoBroadcastJoinThreshold: {spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")}')\n",
    "print(f'Current value of adaptive.autoBroadcastJoinThreshold: {spark.conf.get(\"spark.databricks.adaptive.autoBroadcastJoinThreshold\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60b20fe8-9d67-4f24-95c9-9cd2e824a829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Perform the join of **transactions**, **stores**, and **countries** to create a table named **transact_countries** without using broadcast joins. Notice that this query takes about ~1 minute to execute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "423ebd19-72b1-4489-8ab6-51e0ab674a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df_no_broadcast = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        transactions.id,\n",
    "        amount,\n",
    "        countries.name as country_name,\n",
    "        employees,\n",
    "        stores.name as store_name\n",
    "    FROM\n",
    "        transactions\n",
    "    LEFT JOIN\n",
    "        stores\n",
    "        ON\n",
    "            transactions.store_id = stores.id\n",
    "    LEFT JOIN\n",
    "        countries\n",
    "        ON\n",
    "            transactions.country_id = countries.id\n",
    "\"\"\")\n",
    "\n",
    "## Create a table with the joined data\n",
    "(joined_df_no_broadcast\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .saveAsTable('transact_countries')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95fcc195-6128-496b-a3de-a7d161ede4f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Open the Spark UI to the Stages page and notice that there were two large **1.4GB** shuffles of data. \n",
    "\n",
    "A shuffle refers to the process of redistributing data across different nodes/partitions. This is necessary for operations like joins, where data from two or more datasets needs to be combined based on a key, but the relevant rows might not be on the same partition.\n",
    "\n",
    "\n",
    "To view how the query DAG, complete the following steps:\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. In the second job, right-click on **View** and select *Open in a New Tab*.\n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment, if you click **View** without opening it in a new tab, the pop-up window will display an error.\n",
    "\n",
    "3. Click on the **Stages** link in the top navigation bar. Notice that in the stages of this specific query, there were very large **Shuffle Read** and **Shuffle Write** operations for the query.\n",
    "\n",
    "    ![1.3-non_broadcast_join_stages.png](./Includes/images/1.3-non_broadcast_join_stages.png)\n",
    "\n",
    "4. Next, in the **Completed Stages** table select the link (**mapPartitionsInternal at...**) in the **Description** column for the row that has the large **Shuffle Read** and **Shuffle Write** the **Jobs** link in the top navigation bar. For example, in the image above it would be row **151**.\n",
    "\n",
    "5. Then, select number link in the table at the top in **Associated Jobs Ids** -> then the query number in **Associated SQL Query**. Here, you should see the entire query plan (DAG).\n",
    "\n",
    "    ![1.3-non_broadcastjoin_dag.png](./Includes/images/1.3-non_broadcastjoin_dag.png)\n",
    "\n",
    "<br></br>\n",
    "6. In the query plan, complete the following steps:\n",
    "<br></br>\n",
    "##### 6a. Shuffle Join with transactions and stores\n",
    "\n",
    "![1.3-shuffle_join_transactions_stores.png](./Includes/images/1.3-shuffle_join_transactions_stores.png)\n",
    "\n",
    "- Find **PhotonScan parquet dbacademy.your-schema-name.transactions (1)**.\n",
    "\n",
    "- Above that table, expand **PhotonShuffleExchangeSink (2)**.\n",
    "\n",
    "- Notice that the shuffle uses around 1.4GB for the first join with the **stores** table.\n",
    "<br></br>\n",
    "##### 6b. Shuffle Join with transactions and stores\n",
    "\n",
    "![1.3-shuffle_join_results_countries.png](./Includes/images/1.3-shuffle_join_results_countries.png)\n",
    "\n",
    "- Find **PhotonShuffleExchangeSource** on the left side above the **AQEShuffleRead** for the result of the previous join.\n",
    "\n",
    "- Expand **PhotonShuffleExchangeSource**.\n",
    "\n",
    "- Notice that another shuffle was performed with around 1.4GB for the join of the results of the first join to the join with the **countries** table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0d3565f-ca69-459f-b3da-5d6010308f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###D2. Enabling Broadcast Join\n",
    "\n",
    "Broadcast join avoids the shuffle. In the above cells, we explicitly turned off broadcast joins, but now we'll return the configuration to the default so that broadcast join is enabled to compare the queries. \n",
    "\n",
    "This will work in this case because at least one of the tables in each join is relatively small and below the thresholds:\n",
    "- < 10MB for a broadcast join without AQE\n",
    "- < 30MB for a broadcast join with AQE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef118ff8-42a4-43e5-b521-0a7db99b16bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the default configurations for broadcast joins\n",
    "spark.conf.unset(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "spark.conf.unset(\"spark.databricks.adaptive.autoBroadcastJoinThreshold\")\n",
    "\n",
    "## View the default values\n",
    "print(f'Current value of autoBroadcastJoinThreshold: {spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")}')\n",
    "print(f'Current value of adaptive.autoBroadcastJoinThreshold: {spark.conf.get(\"spark.databricks.adaptive.autoBroadcastJoinThreshold\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3ceb070-1a0e-4aaf-9f32-176ab9942e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the query to perform the same join as before. Notice that this query executes in about half the time of the previous join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a42209b7-0bf0-4648-82af-fa6df1276b33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        transactions.id,\n",
    "        amount,\n",
    "        countries.name as country_name,\n",
    "        employees,\n",
    "        stores.name as store_name\n",
    "    FROM\n",
    "        transactions\n",
    "    LEFT JOIN\n",
    "        stores\n",
    "        ON\n",
    "            transactions.store_id = stores.id\n",
    "    LEFT JOIN\n",
    "        countries\n",
    "        ON\n",
    "            transactions.country_id = countries.id\n",
    "\"\"\")\n",
    "\n",
    "(joined_df\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .saveAsTable('transact_countries')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8944f26c-8ece-4ea2-9c17-179124f8a21a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "This is an improvement. Referring back to the Spark UI **Stages** tab, note that there are no large shuffle reads anymore. Only the small tables were shuffled, but the large table was not, so we avoided moving 1.4GB twice.\n",
    "\n",
    "\n",
    "View the stages of this query. Notice that the large shuffles have been avoided, improving the query efficiency.\n",
    "![1.3-broadcast_join_stages.png](./Includes/images/1.3-broadcast_join_stages.png)\n",
    "\n",
    "<br>\n",
    "You can also view the query plan and see that the large tables were not shuffled.\n",
    "\n",
    "![1.3-broadcast_join.png](./Includes/images/1.3-broadcast_join.png)\n",
    "\n",
    "<br>\n",
    "Broadcast joins can be significantly faster than shuffle joins if one of the tables is very large and the other is small. Unfortunately, broadcast joins only work if at least one of the tables is less than 100MB in size. In the case of joining larger tables, if we want to avoid the shuffle, we may need to reconsider our schema to avoid having to do the join in the first place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ba18254-ebd7-4c7b-a536-2ea79770bb1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Aggregations\n",
    "\n",
    "Aggregations also use a shuffle, but they're often much less expensive. The following cell executes a query that demonstrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f661f6e-4c46-47c4-a695-8acbc8134e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  country_id, \n",
    "  COUNT(*) AS count,\n",
    "  AVG(amount) AS avg_amount\n",
    "FROM transactions\n",
    "GROUP BY country_id\n",
    "ORDER BY count DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ac30a82-6956-4a61-8ac1-9218eb9a43a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "That was fast! There are a lot of things going on here. One of the main points is that we're only shuffling the counts and sums necessary to compute the counts and averages that were requested. This only results in shuffling a few KB. Use the Spark UI once again to verify this.\n",
    "\n",
    "![1.3_aggregations-ui.png](./Includes/images/1.3-aggregations_ui.png)\n",
    "\n",
    "So the shuffle is cheap compared to the shuffle joins, which need to shuffle all of the data. It also helps that our output is basically 0 in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dbb9db4-d465-4902-9c39-7ab9d7e3940c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "PO 1.3 - Shuffle",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6880b06e-69f6-42f2-bbfe-b6270c11a5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93b223bc-c38f-4ab3-8906-bc427c59723c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Streaming ETL Lab\n",
    "\n",
    "This notebook programmatically generates and triggers an update of a Lakeflow Spark Declarative pipeline that consists of the following notebooks:\n",
    "\n",
    "|Lakeflow Spark Declarative pipeline|\n",
    "|---|\n",
    "|Auto Load to Bronze|\n",
    "|Stream from Multiplex Bronze|\n",
    "|Quality Enforcement|\n",
    "|[Streaming ETL]($./Pipeline/SDLT 2.4.1L - Streaming ETL Lab)|\n",
    "\n",
    "As we continue through the course, you can return to this notebook and use the provided methods to:\n",
    "- Land a new batch of data\n",
    "- Trigger a pipeline update\n",
    "- Process all remaining data\n",
    "\n",
    "**NOTE:** Re-running the entire notebook will delete the underlying data files for both the source data and your Lakeflow Spark Declarative pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ac908d3-0350-42df-a109-e8755ef4509a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b447590-03a4-4c3e-ae27-9ade334de62e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Setup\n",
    "Run the following cell to reset and configure your working environment for this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b4df1ae-0539-41b4-9eb1-08e53904e2c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-04L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "116fc75e-2034-4be6-9869-72cbdbe9c5af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Generate the Lakeflow Spark Declarative pipeline\n",
    "Run the cell below to auto-generate the pipeline using the provided configuration values. Please navigate to Pipelines under Data Engineering Section.\n",
    "\n",
    "**NOTE:** `DeclarativePipelineCreator` is a custom Python class provided in this course to simplify pipeline creation and execution. It leverages the Databricks REST API and SDK behind the scenes. While these APIs are beyond the scope of this course, you can explore the class implementation in the `./Includes/Classroom-Setup-Common` notebook if you're interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b86390d-b113-4fa5-94a0-0b466363065b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "demo_pipeline = DeclarativePipelineCreator(\n",
    "    pipeline_name=f'demo_04_pipeline_{DA.schema_name}', \n",
    "    catalog_name='dbacademy',\n",
    "    schema_name=DA.schema_name,\n",
    "    root_path_folder_name='Pipeline',\n",
    "    source_folder_names=[\n",
    "        'SDLT 2.1.1 - Auto Load to Bronze',\n",
    "        'SDLT 2.2.1 - Stream from Multiplex Bronze',\n",
    "        'SDLT 2.3.1 - Data Quality Enforcement',\n",
    "        'SDLT 2.4.1L - Streaming ETL Lab'\n",
    "        ],\n",
    "    configuration={\"source\": DA.paths.stream_source, \"lookup_db\": DA.lookup_db},\n",
    "    serverless=True\n",
    ")\n",
    "\n",
    "demo_pipeline.create_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1d7fe1d-7805-4856-a1b7-f1d354200c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Trigger Pipeline Run\n",
    "\n",
    "With a pipeline created, you will now run the pipeline. The initial run will take several minutes while a cluster is provisioned. Subsequent runs will be appreciably quicker.\n",
    "\n",
    "Explore the DAG - As the pipeline completes, the execution flow is graphed. With each triggered update, all newly arriving data will be processed through your pipeline. Metrics will always be reported for current run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efe3cff3-b23b-4460-a1b8-d11db3b15af1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### REQUIRED!!!: Make sure to complete the lab in the Lakeflow Spark Declarative pipeline notebook linked above ([Streaming ETL Lab]($./Pipeline/SDLT 2.4.1L - Streaming ETL Lab)) before running this pipeline.\n",
    "\n",
    "After editing the final notebook, run the cell below to start the pipeline.\n",
    "\n",
    "**NOTE:** If prompted to enable the new Lakeflow Spark Declarative pipeline editor, you can choose to enable it if you're already familiar with it or if you'd like to explore the new interface.\n",
    "\n",
    "**\uD83D\uDCCC NOTE: The pipeline may fail because the catalog used in previous pipelines is still in use. If this happens, please wait for 1â€“2 minutes and then rerun the pipeline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3c913e4-cadc-4131-b13a-33d1c04e2dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "demo_pipeline.start_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46ece369-1e31-4c22-9aa4-eae3f095cbd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### \uD83D\uDCCC NOTE: Please navigate to the Jobs and Pipelines tab and ensure the pipeline has completed successfully before running further cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a300fdc-802a-4055-a810-778f74043dfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Land New Data\n",
    "\n",
    "Run the cell below to land more data in the source directory, then manually trigger another pipeline update using the UI or the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15a3aa43-e7c5-4ee5-a748-953333f1bd82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.daily_stream.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eccdf95-1b03-4cc5-8c2b-9aa632687e47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Process All Remaining Data\n",
    "\n",
    "To continuously load all remaining batches of data to the source directory, call the same load method above with the **`continuous`** parameter set to **`True`**.\n",
    "\n",
    "Trigger another update to process the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0355246-493a-4800-8784-b3e80c7a707e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.daily_stream.load(continuous=True)  # Load all remaining batches of data\n",
    "demo_pipeline.start_pipeline()  # Trigger another pipeline update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa84bf64-1b58-433d-85bd-202f8a2e44d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "SDLT 2.4L - Streaming ETL Lab",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
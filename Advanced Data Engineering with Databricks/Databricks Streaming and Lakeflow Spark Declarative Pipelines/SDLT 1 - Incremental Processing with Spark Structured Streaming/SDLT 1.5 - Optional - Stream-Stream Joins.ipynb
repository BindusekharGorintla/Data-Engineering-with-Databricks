{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5422e079-a304-4347-b9a0-8cd939823963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff532f54-5c3d-42e6-8906-1c625d59a64e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# (Optional) - Stream-Stream Joins\n",
    "\n",
    "##### Objectives\n",
    "1. Create streams using the Rate source\n",
    "1. Perform Stream-Stream Inner Join without Watermarking\n",
    "1. Perform Stream-Stream Inner Join with Watermarking\n",
    "2. Perform Stream-Stream Inner Join with Watermarking and Event Time Constraints\n",
    "\n",
    "##### Classes\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html\" target=\"_blank\">DataStreamReader</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87c0bad9-695b-48ae-8bac-c87bbfe80c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4032f13f-cef7-488b-88e5-86733ab77731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Stream-Stream Joins\n",
    "We are going to use the canonical example of ad monetization, where we want to find out which ad impressions led to user clicks. \n",
    "Typically, in such scenarios, there are two streams of data from different sources - ad impressions and ad clicks. \n",
    "\n",
    "Both type of events have a common ad identifier (say, `adId`), and we want to match clicks with impressions based on the `adId`. \n",
    "In addition, each event also has a timestamp, which we will use to specify additional conditions in the query to limit the streaming state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f808fc3-829c-42fa-a64f-3daf9c21bbc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create two streams - Impressions and Clicks\n",
    "\n",
    "We simulate live streams in a lab setup by using the built-in `rate` format, that generates data at a given fixed rate. \n",
    "\n",
    "See   <a href=\"https://spark.apache.org/docs/3.5.7/structured-streaming-programming-guide.html#input-sources\" target=\"_blank\">Stream Input Sources</a>  for more information on stream sources for data generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d856db1-66c7-4c59-81d5-514377d06bc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "\n",
    "impressions = (\n",
    "  spark\n",
    "    .readStream.format(\"rate\").option(\"rowsPerSecond\", \"5\").option(\"numPartitions\", \"1\").load()\n",
    "    .selectExpr(\"value AS adId\", \"timestamp AS impressionTime\")\n",
    ")\n",
    "\n",
    "clicks = (\n",
    "  spark\n",
    "  .readStream.format(\"rate\").option(\"rowsPerSecond\", \"5\").option(\"numPartitions\", \"1\").load()\n",
    "  .where((rand() * 100).cast(\"integer\") < 10)      # 10 out of every 100 impressions result in a click\n",
    "  .selectExpr(\"(value - 50) AS adId \", \"timestamp AS clickTime\")      # -50 so that a click with same id as impression is generated later (i.e. delayed data).\n",
    "  .where(\"adId > 0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c24bb9a5-f041-4710-8854-06dd1db46d6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see what data these two streaming DataFrames generate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e21dffd3-b87a-4224-ba26-b39ecc6bfed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(impressions, streamName=\"display_impressions\")\n",
    "\n",
    "##################################\n",
    "## Once finished viewing, click  ##\n",
    "## 'Interrupt' before proceeding ##\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48cb8ed0-1cfb-4d26-bf10-c5f9fc9fdf6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(clicks, streamName=\"display_clicks\")\n",
    "\n",
    "##################################\n",
    "## Once finished viewing, click  ##\n",
    "## 'Interrupt' before proceeding ##\n",
    "##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f512b8fc-f8f6-4019-b585-f65641df82de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stream-Stream Inner Join without Watermark\n",
    "\n",
    "Let's join these two data streams. This is exactly the same as joining two batch DataFrames/Datasets by their common key `adId`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "043bab04-a5ca-4a6d-a1ee-3071a1d3c33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "## Without Watermark, State continues to grow ##\n",
    "################################################\n",
    "\n",
    "display(impressions.join(clicks, \"adId\"), streamName=\"naive_streaming_join\")\n",
    "\n",
    "###################################\n",
    "## Once finished viewing, click  ##\n",
    "## 'Interrupt' before proceeding ##\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e8a6071-8018-4e23-9a7e-4f99cfdc128e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After you start this query, within a minute, you will start getting joined impressions and clicks. The delays of a minute is due to the fact that clicks are being generated with delay over the corresponding impressions.\n",
    "\n",
    "In addition, if you expand the details of the query above, you will find a few timelines of query metrics - the processing rates, the micro-batch durations, and the size of the state. \n",
    "If you keep running this query, you will notice that the state will keep growing in an unbounded manner. This is because the query must buffer all past input as any new input can match with any input from the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "deb0767c-7e71-4365-953e-8a704fdbb851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Stream-Stream Inner Join with Watermarking\n",
    "\n",
    "To avoid unbounded state, you have to define additional join conditions such that indefinitely old inputs cannot match with future inputs and therefore can be cleared from the state. In other words, you will have to do the following additional steps in the join.\n",
    "\n",
    "1. Define watermark delays on both inputs such that the engine knows how delayed the input can be. \n",
    "\n",
    "1. Define a constraint on event-time across the two inputs such that the engine can figure out when old rows of one input is not going to be required (i.e. will not satisfy the time constraint) for matches with the other input. This constraint can be defined in one of the two ways.\n",
    "\n",
    "  a. Time range join conditions (e.g. `...JOIN ON leftTime BETWEEN rightTime AND rightTime + INTERVAL 1 HOUR`),\n",
    "\n",
    "  b. Join on event-time windows (e.g. `...JOIN ON leftTimeWindow = rightTimeWindow`).\n",
    "\n",
    "Let's apply these steps to our use case. \n",
    "\n",
    "1. Watermark delays: Say, the impressions and the corresponding clicks can be delayed/late in event-time by at most \"10 seconds\" and \"20 seconds\", respectively. This is specified in the query as watermarks delays using `withWatermark`.\n",
    "\n",
    "1. Event-time range condition: Say, a click can occur within a time range of 0 seconds to 1 minute after the corresponding impression. This is specified in the query as a join condition between `impressionTime` and `clickTime`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38b0d798-096f-4e6a-9296-58cb04d167b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Define watermarks\n",
    "impressionsWithWatermark = (impressions \n",
    "  .selectExpr(\"adId AS impressionAdId\", \"impressionTime\") \n",
    "  .withWatermark(\"impressionTime\", \"10 seconds \"))\n",
    "                            \n",
    "clicksWithWatermark = (clicks \n",
    "  .selectExpr(\"adId AS clickAdId\", \"clickTime\")\n",
    "  .withWatermark(\"clickTime\", \"20 seconds\"))        # max 20 seconds late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04f9976a-7136-4c61-a7ea-2b6b0af8ea15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inner join with Watermark \n",
    "display(impressionsWithWatermark.join(\n",
    "    clicksWithWatermark,\n",
    "    expr(\"\"\" clickAdId = impressionAdId\"\"\")), streamName=\"streaming_join_with_watermarks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "268e5aac-ef03-40e9-ae4b-7dc5f8c088a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stream-Stream Join with Watermark and Event Time Constraint\n",
    "\n",
    "This will enable Structured Streaming to perform full state cleanup. Use this for long-running stream processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "696fcc5b-a20c-4775-9c76-92a81b6db94a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inner join with watermark + Time conditions - Required for full state cleanup\n",
    "display(impressionsWithWatermark.join(\n",
    "    clicksWithWatermark,\n",
    "    expr(\"\"\" clickAdId = impressionAdId AND \n",
    "      clickTime >= impressionTime AND \n",
    "      clickTime <= impressionTime + interval 1 minutes\"\"\")), streamName=\"streaming_join_with_watermarks_and_event_time_constraints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "792b4e46-1d91-4d5a-b3e0-dec77daa6598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are getting the similar results as the previous simple join query. However, if you look at the query metrics now, you will find that after about a couple of minutes of running the query, the size of the state will stabilize as the old buffered events will start getting cleared up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab2fc9ad-79a8-4025-9b15-0aed2db7dcee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Further Information\n",
    "You can read more about stream-stream joins in the following places:\n",
    "\n",
    "- Databricks blog post on stream-stream joins - https://databricks.com/blog/2018/03/13/introducing-stream-stream-joins-in-apache-spark-2-3.html\n",
    "- Apache Programming Guide on Structured Streaming - https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#stream-stream-joins\n",
    "- Talk at Spark Summit Europe 2017 - https://databricks.com/session/deep-dive-into-stateful-stream-processing-in-structured-streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e3a268f-68c0-43bd-81e0-62b585a01565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for s in spark.streams.active:\n",
    "    print(s.name)\n",
    "    s.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccfc3f50-32f9-481c-b588-c92e28616688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "SDLT 1.5 - Optional - Stream-Stream Joins",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59632b42-1e83-4bb0-ad6b-69cad6a5257d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2995ef3b-f288-4e03-bdc6-0310e9392ae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab - Creating Bronze Tables from CSV Files\n",
    "### Duration: ~15-20 minutes\n",
    "\n",
    "This lab is divided into two sections: the **course lab** and an **optional challenge**. \n",
    "\n",
    "In a live class, if you finish the main lab early, feel free to attempt the challenge. You can also complete the challenge after class. \n",
    "\n",
    "**NOTE:** The challenge will require you to use resources outside the scope of this course.\n",
    "\n",
    "### Learning Objectives\n",
    "  - Inspect CSV files.\n",
    "  - Read in CSV files as a Delta table and append metadata columns.\n",
    "  - Record malformed data types using the `_rescued_data` column.  \n",
    "\n",
    "### Challenge Learning Objectives:\n",
    "  - Clean malformed data types during ingestion by leveraging the `_rescued_data` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd7dc00-3715-4d17-b278-356cb60473dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default and you have a Shared SQL warehouse.\n",
    "\n",
    "<!-- ![Select Cluster](./Includes/images/selecting_cluster_info.png) -->\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4285c9ae-3dae-4eef-9747-543bf8a9592c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Course Lab - Creating Bronze Tables from CSV Files\n",
    "\n",
    "### Scenario\n",
    "\n",
    "You are working with your data team on ingesting a CSV file into Databricks. However, you notice there is a malformed row in your CSV file. Your job is to ingest the file and rescue the malformed data and write as a Delta table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99db942c-25ee-4a40-8f0d-b3c5c9269a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this notebook.\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course in the lab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a963733-ed72-4afd-840a-f56bb6e3bd80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-05L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b1732ac-05a1-477e-8bdf-e03d88b11e98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to view your default catalog and schema. Notice that your default catalog is **dbacademy** and your default schema is your unique **labuser** schema.\n",
    "\n",
    "**NOTE:** The default catalog and schema are pre-configured for you to avoid the need to specify the three-level name when writing your tables (i.e., catalog.schema.table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e052e4e1-cf45-43c8-9c16-8a38fd8297a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09023e02-0e0f-402b-98bf-812380579dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Lab - CSV File Ingestion\n",
    "Ingest the CSV file as a Delta table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a453a23d-f8fe-4d56-a18f-8977b3c7da6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B1. Inspect the Dataset\n",
    "\n",
    "1. In the cell below, view the value of the SQL variable `DA.paths_working_dir`. This variable will reference your **labuser** volume, as each user has a different source volume. This variable is created within the classroom setup script to dynamically reference your unique volume.\n",
    "\n",
    "   Run the cell and review the results. You’ll notice that the `DA.paths_working_dir` variable points to your `/Volumes/dbacademy/ops/labuser` volume.\n",
    "\n",
    "**Note:** Instead of using the `DA.paths_working_dir` variable, you could also specify the path name directly by right clicking on your volume and selecting **Copy volume path**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d39f1fcb-3bea-4ffa-ba7b-f7a4b37d4c20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "values(DA.paths_working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7b44812-d418-45fd-93e1-46a8fa31ef1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. You can concatenate the `DA.paths_working_dir` SQL variable with a string to specify a specific subdirectory in your volume.\n",
    "\n",
    "   Run the cell below and review the results. You’ll notice that it returns the path to your **lab_malformed_data.csv** file. This method will be used when referencing your volume within the `read_files` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a27f3bcb-89bd-4571-b721-8b2800ac5b49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "values(DA.paths_working_dir || '/csv_demo_files/lab_malformed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "091923e2-6984-47f8-9acc-4502f61a6bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Next, let's take a look at our CSV file. \n",
    "\n",
    "    Copy the path from the output below and paste it within the backticks in the query to reference the **lab_malformed_data.csv** file.\n",
    "\n",
    "    The query should use `text.<path_from_above>` to return the headers and rows from the CSV file. \n",
    "\n",
    "    Run the cell and view row 4. Notice that the value for the price contains a `$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6923166b-d710-465a-acb5-f817c8ff5383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "spark.sql(f'''\n",
    "SELECT *\n",
    "FROM text.`{DA.paths.working_dir}/csv_demo_files/lab_malformed_data.csv`\n",
    "'''\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04240bc4-3bed-4868-8879-56667cde9fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B2. Ingesting and Rescuing Malformed Data\n",
    "\n",
    "Begin developing your query to ingest the CSV file in the specified path and view malformed records using the **_rescued_data** column.\n",
    "\n",
    "#### Requirements\n",
    "Your final SQL query should ingest the CSV file using CTAS and `read_files`. **In the cell below, do not create a table yet. Simply start developing your query to ingest and create the table**:\n",
    "\n",
    "1. Select all columns from the raw CSV file.\n",
    "\n",
    "2. Use the `read_files()` function with appropriate options to read the CSV file. \n",
    "   - **HINT:** Note that the delimiter is a comma (`,`) not a pipe (`|`).\n",
    "\n",
    "3. Explicitly define the schema for ingestion. The schema is defined as follows:  \n",
    "   - `item_id` (STRING)  \n",
    "   - `name` (STRING)  \n",
    "   - `price` (DOUBLE)\n",
    "\n",
    "4. Use the correct option to include the rescued data column and name it **_rescued_data** to capture malformed rows.\n",
    "\n",
    "   - **HINT**: If you define a schema you must [use the rescuedDataColumn option](https://docs.databricks.com/aws/en/sql/language-manual/functions/read_files#csv-options) to add the **_rescued_data** column.\n",
    "\n",
    "**SOLUTION OUTPUT**\n",
    "\n",
    "Your output result should look like the following:\n",
    "| item_id (STRING)   | name (STRING)                   | price (DOUBLE)| _rescued_data (STRING)                                                                                         |\n",
    "|-----------|-------------------------|-------|-----------------------------------------------------------------------------------------------------------|\n",
    "| M_PREM_Q  | Premium Queen Mattress   | 1795  | null                                                                                                      |\n",
    "| M_STAN_F  | Standard Full Mattress   | 945   | null                                                                                                      |\n",
    "| M_PREM_A  | Premium Queen Mattress   | null  | {\"price\":\"$100.00\",\"_file_path\":\"dbfs:/Volumes/dbacademy/ops/peter_s@databricks_com/csv_demo_files/lab_malformed_data.csv\"} |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa838a36-61fe-4d5a-9c7e-2515cd5c5a15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM read_files(\n",
    "        DA.paths_working_dir || '/csv_demo_files/lab_malformed_data.csv',\n",
    "        format => \"csv\",\n",
    "        sep => \",\",\n",
    "        header => true,\n",
    "        schema => '''\n",
    "              item_id STRING, \n",
    "              name STRING, \n",
    "              price DOUBLE\n",
    "        ''',\n",
    "        rescueddatacolumn => \"_rescued_data\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0ba9ec7-19f1-4955-96f6-ee0cee3c61a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B3. Add Additional Metadata Columns During Ingestion\n",
    "\n",
    "Next, you can create the final bronze table named **05_lab_bronze** that contains the additional metadata columns. Use the query you created above as the starting point.\n",
    "\n",
    "### Final Table Requirements\n",
    "\n",
    "Incorporate the SQL query you created in the previous section and complete the following:\n",
    "\n",
    "1. Use a CTAS statement to create the final bronze Delta table named **05_lab_bronze**. \n",
    "\n",
    "1. Ingest the same CSV file `/Volumes/dbacademy/ops/<username>/csv_demo_files/lab_malformed_data.csv`\n",
    "\n",
    "1. Use the same defined schema:  \n",
    "   - `item_id` (STRING)  \n",
    "   - `name` (STRING)  \n",
    "   - `price` (DOUBLE)\n",
    "\n",
    "1. Use the `_metadata` column to create two new columns named **file_modification_time** and **source_file**  within your SELECT statement.\n",
    "   - **HINT:** [_metadata](https://docs.databricks.com/en/ingestion/file-metadata-column.html)\n",
    "\n",
    "1. Add a column named **ingestion_time** that provides a timestamp for ingestion. \n",
    "   - **HINT:** Use the [current_timestamp()](https://docs.databricks.com/aws/en/sql/language-manual/functions/current_timestamp) to record the current timestamp at the start of the query evaluation. \n",
    "\n",
    "\n",
    "**NOTE:** If you need to see how your final table should look, run the next cell to view the solution table **05_lab_solution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddb2fe76-e2d6-440a-84c5-1947ef37da16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Run if you want to see the final table\n",
    "SELECT * \n",
    "FROM 05_lab_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5665aff1-2e71-4d96-acbe-f3dd10fff805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "---- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS 05_lab_bronze;\n",
    "\n",
    "---- Create the Delta table\n",
    "CREATE TABLE 05_lab_bronze \n",
    "AS\n",
    "SELECT\n",
    "  *,\n",
    "  _metadata.file_modification_time AS file_modification_time,\n",
    "  _metadata.file_name AS source_file, \n",
    "  current_timestamp() as ingestion_time\n",
    "FROM read_files(\n",
    "        DA.paths_working_dir || '/csv_demo_files/lab_malformed_data.csv',\n",
    "        format => \"csv\",\n",
    "        sep => \",\",\n",
    "        header => true,\n",
    "        schema => 'item_id STRING, name STRING, price DOUBLE', \n",
    "        rescueddatacolumn => \"_rescued_data\"\n",
    "      );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5dfcd50-6a99-4948-bfa3-ce107f676da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to view your final table **05_lab_bronze** and compare it with the solution table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fe68d81-f959-4cf0-a6cc-3968a2fd245e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- View the final table\n",
    "SELECT * \n",
    "FROM 05_lab_bronze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc36b3a8-f821-434b-887e-ed45e0172852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. (Optional) Challenge: Rescuing Data\n",
    "The challenge is optional during a live class if you have time. This challenge may require you to use documentation. If you run out of time during a live class, try and complete this after class.\n",
    "\n",
    "#### Optional Challenge Scenario\n",
    "You report back to your data team and everyone agrees to clean up any values in the rescued data column that contain a `$` when ingesting as a bronze table. To fix this issue, you agree to handle these edge case during ingestion by leveraging the `_rescued_data` column.\n",
    "\n",
    "Your team has decided to strip the `$` in the price and simply store the numeric value during ingestion.\n",
    "\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "- Complete the SQL query below to modify the **_rescued_data** column and correctly fixes the malformed value.\n",
    "\n",
    "- Your final table should return the following columns:\n",
    "  - **item_id**\n",
    "  - **name**\n",
    "  - **price** (the original price column)\n",
    "  - **TO DO**: **price_fixed** (fix the malformed row in the rescued data column `$100` and return all prices a numeric values)\n",
    "  - **_rescued_data**\n",
    "  - **source_file**\n",
    "  - **file_modification_time**\n",
    "  - **ingestion_timestamp**\n",
    "\n",
    "- Use `CREATE TABLE AS` with `read_files()` to read the CSV file and create a table named .\n",
    "\n",
    "**HINT:** One solution you can use is the [`COALESCE()`](https://docs.databricks.com/aws/en/sql/language-manual/functions/coalesce) function along with the [`REPLACE()`](https://docs.databricks.com/aws/en/sql/language-manual/functions/replace) function to replace the malformed string price (`$100`) with `100`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b44455c-0ff9-470c-8a21-48148cca3576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can view the final table by running the next cell if you would like a hint for building your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01b856eb-3885-47a2-b387-d36e786f0a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Run if you want to see the final table\n",
    "SELECT * \n",
    "FROM 05_lab_challenge_solution;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb2b385f-557e-4098-b4d2-9db923fbfc08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE 05_lab_challenge\n",
    "SELECT\n",
    "  item_id,\n",
    "  name,\n",
    "  price,\n",
    "  coalesce(price, replace(_rescued_data:price,'$','')) AS price_fixed,\n",
    "  _rescued_data,\n",
    "  _metadata.file_modification_time AS file_modification_time,\n",
    "  _metadata.file_name AS source_file, \n",
    "  current_timestamp() as ingestion_timestamp\n",
    "FROM read_files(\n",
    "        DA.paths_working_dir || '/csv_demo_files/lab_malformed_data.csv',\n",
    "        format => \"csv\",\n",
    "        sep => \",\",\n",
    "        header => true,\n",
    "        schema => 'item_id STRING, name STRING, price DOUBLE', \n",
    "        rescueddatacolumn => \"_rescued_data\"\n",
    "      );\n",
    "\n",
    "---- Display the table\n",
    "SELECT *\n",
    "FROM 05_lab_challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ff938a2-106e-41a7-aac2-f215fe3a27d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "05L Solution - Creating Bronze Tables from CSV Files",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
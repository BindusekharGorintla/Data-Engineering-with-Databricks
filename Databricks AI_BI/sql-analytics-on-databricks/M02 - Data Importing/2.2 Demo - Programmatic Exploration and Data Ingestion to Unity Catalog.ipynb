{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8eb517c-d658-41a0-bb7a-cae53068591a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d32082de-dfcb-40ac-a298-213d3b5d6181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.2 Demo - Programmatic Exploration and Data Ingestion to Unity Catalog\n",
    "\n",
    "In this demonstration, we will programmatically explore our data objects, display a raw CSV file from a volume, then read the CSV file from the volume and create a table.\n",
    "\n",
    "### Objectives\n",
    "- Apply programmatic techniques to view data objects in our environment.\n",
    "- Demonstrate how to upload a CSV file into a volume in Databricks.\n",
    "- Demonstrate how to use a `CREATE TABLE` statement to create a table from a CSV file in a Databricks volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "203910c9-9c0a-4808-835c-6c1fd637eea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT A SHARED SQL WAREHOUSE\n",
    "\n",
    "Before executing cells in this notebook, please select the **SHARED SQL WAREHOUSE** in the lab. Follow these steps:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down to select compute (it might say **Connect**). Complete one of the following below:\n",
    "\n",
    "   a. Under **Recent resources**, check to see if you have a **shared_warehouse SQL**. If you do, select it.\n",
    "\n",
    "   b. If you do not have a **shared_warehouse** under **Recent resources**, complete the following:\n",
    "\n",
    "    - In the same drop-down, select **More**.\n",
    "\n",
    "    - Then select the **SQL Warehouse** button.\n",
    "\n",
    "    - In the drop-down, make sure **shared_warehouse** is selected.\n",
    "\n",
    "    - Then, at the bottom of the pop-up, select **Start and attach**.\n",
    "\n",
    "<br></br>\n",
    "   <img src=\"../Includes/images/sql_warehouse.png\" alt=\"SQL Warehouse\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac529cb8-6b4f-445a-b6d5-9048758790a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this notebook.\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course in the lab environment.\n",
    "\n",
    "### IMPORTANT LAB INFORMATION\n",
    "\n",
    "Recall that your lab setup is created with the [0 - REQUIRED - Course Setup and Data Discovery]($../0 - REQUIRED - Course Setup and Data Discovery) notebook. If you end your lab session or if your session times out, your environment will be reset, and you will need to rerun the Course Setup notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "070433f1-4447-4c20-9e0b-258af408a3df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/2.2-Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07d33b14-7907-466d-a37f-ceafa7496bd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Programmatically Exploring Your Environment\n",
    "\n",
    "In this section, we will demonstrate how to programmatically explore your environment, an alternative method to using the Catalog Explorer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64ab1912-7157-4b7c-aff4-53f359583965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. View your available catalogs using the `SHOW CATALOGS` statement. Notice that your environment has a series of catalogs available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42a7f00e-b68b-43f0-ab82-a800cf44c740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SHOW CATALOGS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f72d91fa-ce06-42e5-a654-01e11b2fef90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the following cell to view your default catalog and schema. You should notice that your default catalog is set to **samples** and your default schema is set to **nyctaxi**.\n",
    "\n",
    "   **NOTE:** Setting the default catalog and schema in Databricks allows you to avoid repeatedly typing the full path (catalog.schema.table) when referring to your data objects. Once set, Databricks will automatically use your chosen catalog and schema, making it easier and faster to work with your data without needing to specify the full namespace each time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aadb515-e671-4db6-946e-dfc12d3d5b8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c6b8e1e-f359-4239-9688-422071f4f62f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Use the `SHOW SCHEMAS` statement to view available schemas within the default catalog. Notice that it displays schemas (databases) within the **samples** catalog since that is your current default catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9aaf255-303e-4634-9d9f-98e436308970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SHOW SCHEMAS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e002311b-e3d2-44a6-8ca2-2abfdbc95463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. You can modify the `SHOW SCHEMAS` statement to specify a specific catalog, like the **dbacademy** catalog. Notice that this displays available schemas (databases) within the **dbacademy** catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60c9a566-309c-437c-9798-311f08d6cfb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SHOW SCHEMAS IN dbacademy;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b3409de-0662-42d4-a9f9-cc376c851bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. To view available tables in a schema, use the `SHOW TABLES` statement. Notice that, by default, it displays the one table within the default **samples** catalog in the **nyctaxi** schema (database).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01dd408e-cd64-4b65-b92c-13ed0e220108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c47aa74-2287-4282-88bb-bcaeac5465a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. To query the **samples.nyctaxi.trips** table, you only need to specify the table name **trips** and not the entire three-level namespace (catalog.schema.table) because the default catalog and schema are **samples** and **nyctaxi**, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc76ba3d-3d57-49ad-a5db-841c40b83c8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM trips\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c461ff8d-6a22-440e-ad51-4a8946d9f5a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Let's try querying the **dbacademy.labuser.ca_orders** table without using the three-level namespace. Notice that an error is returned because it is looking for the **ca_orders** table in **samples.nyctaxi**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9509be98-fd21-4946-b422-be42981fcd41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- This query will return an error \n",
    "-- This is because the table does not exist in the default catalog (samples) and default schema (nyctaxi) schema\n",
    "SELECT *\n",
    "FROM ca_orders\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98844690-1caa-4ab2-9ffb-ce643351825b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. We want to modify our default catalog and default schema to use **dbacademy** and our **labuser** schema to avoid writing the three-level namespace everytime we query and create tables in this course.\n",
    "\n",
    "    However, before we proceed, note that each of us has a different schema name. Your specific schema name has been stored dynamically in the SQL variable `DA.schema_name` during the classroom setup script.\n",
    "\n",
    "    Run the code below and confirm that the value of the `DA.schema_name` variable matches your specific schema name (e.g., **labuser1234_678**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a9646a8-8185-422f-b2d1-832ecae6a574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "values(DA.schema_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9662b514-3dab-4ec0-ade3-1ed1f18720c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. Let's modify our default catalog and schema using the `USE CATALOG` and `USE SCHEMA` statements.\n",
    "\n",
    "    - `USE CATALOG` – Sets the current catalog.\n",
    "\n",
    "    - `USE SCHEMA` – Sets the current schema. \n",
    "\n",
    "    **NOTE:** Since our dynamic schema name is stored in the variable `DA.schema_name` as a string, we will need to use the `IDENTIFIER` clause to interpret the constant string in our variable as a schema name. The `IDENTIFIER` clause can interpret a constant string as any of the following:\n",
    "    - Relation (table or view) name\n",
    "    - Function name\n",
    "    - Column name\n",
    "    - Field name\n",
    "    - Schema name\n",
    "    - Catalog name\n",
    "\n",
    "    [IDENTIFIER clause documentation](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-names-identifier-clause?language=SQL)\n",
    "\n",
    "  Alternatively, you can simply add your schema name without using the `IDENTIFIER` clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc773162-0a8f-4e63-ad6b-6396d30f1de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA IDENTIFIER(DA.schema_name);\n",
    "\n",
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4dbfcdf-e989-47b8-862a-83408e6c4b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Let's view the available tables in the **dbacademy** catalog within our **labuser** schema. Notice that your schema contains a variety of tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f47f0b2-92cf-4237-9ae4-789d8732796d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4520d14a-3094-4672-9172-fbf08f02602a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "11. Let's query the **ca_orders** table in the **dbacademy** catalog within our **labuser** schema without using the three-level namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16809599-700a-4f85-8ca3-e7205272b83f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM ca_orders\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64055546-2bfa-4935-8eee-8d6ef0c5e813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12. While you can set your default catalog and schema to avoid using the three-level namespace, there are times when you might want to reference a specific table. You can do this by specifying the catalog and schema name in the query.\n",
    "\n",
    "    In this example, let's query the **samples.nyctaxi.trips** table using the three-level namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eeaaba0-a2cd-42eb-9d33-4f00d71e34f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM samples.nyctaxi.trips\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "639d3de3-99e1-4759-9c17-92897a833751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "13. Lastly, let's view the available volumes within our **labuser** schema in the **dbacademy** catalog using the `SHOW VOLUMES` statement. Notice that our **labuser** schema contains a variety of Databricks volumes, including the **backup** volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b8e896f-ca0c-4f26-a6f2-d9f2b2c4f079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SHOW VOLUMES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e8df906-409b-4717-94b4-0f2567362e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "14. Let's view the available files in our **dbacademy.labuser.backup** volume using the UI. Complete the following:\n",
    "\n",
    "    a. In the left navigation pane, expand the **dbacademy** catalog.\n",
    "\n",
    "    b. Expand your **labuser** schema.\n",
    "\n",
    "    c. Expand **Volumes**.\n",
    "\n",
    "    d. Expand the **backup** volume.\n",
    "\n",
    "    e. Notice that your volume contains the **au_products.csv** file.\n",
    "\n",
    "  **NOTES:** \n",
    "  - You can also use the [LIST statement](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-aux-list) to programmatically view available files in a volume shown below.\n",
    "  - The `LIST` statement does not list files in Unity Catalog managed tables.\n",
    "  - Add your unique schema name to the string in the `LIST` statement to programmatically view the file in the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "290873e1-62af-433c-b97c-08ef2a1a1b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Add your schema name in the LIST statement below, example - `/Volumes/dbacademy/labuser1234_5678/backup`\n",
    "LIST '/Volumes/dbacademy/<ADD_SCHEMA_NAME>/backup'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6eaca59-2cf3-4abc-ac82-6ce9dd42d0ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Create a Table From a CSV File in a Volume\n",
    "\n",
    "In this section, we will use SQL to create a table (Delta Table) from a CSV file stored in a Databricks volume using two methods:\n",
    "- `read_files`\n",
    "- `COPY INTO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c96998a-6bcc-4fae-bc28-02c29a9086bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Our goal is to read the **au_products.csv** file and create a table. To start, it's good practice to examine the raw file(s) you want to use to create a table. We can do that with the following code to [query the data by path](https://docs.databricks.com/aws/en/query#query-data-by-path). This enables us to see:\n",
    "\n",
    "    - The delimiter of the CSV file\n",
    "\n",
    "    - The general structure of the CSV file\n",
    "\n",
    "    - If the CSV contains headers in the first row\n",
    "\n",
    "    - You can use this technique to query a variety of file types.\n",
    "\n",
    "    Complete the following:\n",
    "\n",
    "      a. In the left navigation pane, navigate to your **backup** volume and find the **au_products.csv** file.\n",
    "\n",
    "      b. In the cell below, place your cursor between the two backticks.\n",
    "\n",
    "      c. In the navigation pane, hover over the **au_products.csv** file and select the `>>` to insert the path of the CSV file between the backticks where it says 'REPLACE WITH YOUR VOLUME PATH'. Yours will look something like this with your unique schema name:\n",
    "\n",
    "\n",
    "      ```SQL\n",
    "      SELECT *\n",
    "      FROM text.`/Volumes/dbacademy/labuser1234_5678/backup/au_products.csv`\n",
    "      ```\n",
    "\n",
    "      d. Run the cell and view the results. Notice that:\n",
    "      - The first row of the CSV file contains a header\n",
    "      - The values are separated by a comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6688e921-a7bb-4a5b-855b-368db1b10ab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Add the volume path to your file: Example - `/Volumes/dbacademy/labuser_1234_5678/backup/au_products.csv`\n",
    "SELECT *\n",
    "FROM text.`REPLACE WITH YOUR VOLUME PATH`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36e83b88-7bfc-4502-b4a3-95cff1398264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. read_files Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37372773-b8d0-4604-8282-1cd347f06634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. In the cell below, let's create a table named **au_products** in the **dbacademy** catalog within your **labuser** schema using the **au_products.csv** file with the [read_files table-valued function](https://docs.databricks.com/aws/en/sql/language-manual/functions/read_files) (TVF).\n",
    "\n",
    "    The `read_files` function reads files from a provided location and returns the data in tabular form. It supports reading JSON, CSV, XML, TEXT, BINARYFILE, PARQUET, AVRO, and ORC file formats.\n",
    "\n",
    "    The `read_files` function below uses the following options:\n",
    "\n",
    "      - The path of the CSV file is created by concatenating the volume path with your schema name, which is stored in the `DA.schema_name` variable. This allows dynamic referencing of the file for your unique lab schema name.\n",
    "\n",
    "      - The `format => 'csv'` option specifies the data file format in the source path. The format is auto-inferred if not provided.\n",
    "\n",
    "      - The `header => true` option specifies that the CSV file contains a header.\n",
    "\n",
    "      - When the schema is not provided, `read_files` attempts to infer a unified schema across the discovered files, which requires reading all the files.\n",
    "\n",
    "**NOTES:**\n",
    "\n",
    "- There are a variety of different options for each file type. You can view the available options for your specific file type in the [Options](https://docs.databricks.com/aws/en/sql/language-manual/functions/read_files#options) documentation.\n",
    "\n",
    "- If a volume contains related files, you can read all of the files into a table by specifying the path of the volume without the file name. In this example, we are specifying the volume path and the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb8ff8d5-41f4-4ad3-b309-1d43e1787cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE au_products AS\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  '/Volumes/dbacademy/' || DA.schema_name || '/backup/au_products.csv',\n",
    "  format => 'csv',\n",
    "  header => true\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8538a6a0-9489-49c5-b1c0-433aa6c7c2ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. View available tables in your **labuser** schema. Notice that a new table named **au_products** was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab3ac7a9-47ca-4e99-b1a9-c9d9a119b2a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7f36f7b-74cd-455c-b70f-2d89d594cd41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Query the **au_products** table within your **labuser** schema and view the results.\n",
    "\n",
    "    Notice the following:\n",
    "    - The table was created from the CSV file successfully.\n",
    "    - A new column named **_rescued_data** was added. This column is provided by default to rescue any data that doesn’t match the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e01dbeff-5e1e-4a72-ae21-a5b457f9c7b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM au_products;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48a04495-f0b4-495e-8e49-bfd14a3f00ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. When the schema is not provided, `read_files` attempts to infer a unified schema across the discovered files, which requires reading all the files which can be inefficient for large files.\n",
    "\n",
    "    For larger files it's more efficient to specify the schema within the `read_files` function. For our small CSV file performance is not an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ff90c70-23f5-44f7-95fd-984016690597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE au_products_with_schema AS\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  '/Volumes/dbacademy/' || DA.schema_name || '/backup/au_products.csv',\n",
    "  format => 'csv',\n",
    "  header => true,\n",
    "  schema => '\n",
    "    productid STRING,\n",
    "    productname STRING,\n",
    "    listprice DOUBLE\n",
    "  '\n",
    ");\n",
    "\n",
    "SELECT *\n",
    "FROM au_products_with_schema;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e02ba677-5750-442f-a3d7-a110e529075a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. COPY INTO\n",
    "Another method to create a table from files is using the `COPY INTO` statement. The `COPY INTO` statement loads data from a file location into a Delta table. This is a retryable and idempotent operation — Files in the source location that have already been loaded are skipped. This is true even if the files have been modified since they were loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69707162-502a-4f25-983a-d916058ba70d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. The cell below shows how to create a table and copy CSV data into it using `COPY INTO`:\n",
    "\n",
    "   a. The `CREATE TABLE` statement creates an empty table with a defined schema (columns `productid`, `productname`, and `listprice`). `COPY INTO` will copy the data into this table.\n",
    "\n",
    "   b. The `COPY INTO` command loads CSV files from the specified path into the created table. \n",
    "\n",
    "   c. `FROM` specifies the volume path to read from. This could also reference an external location.\n",
    "\n",
    "    - The `FILEFORMAT = CSV` specifies the format of the input data. Databricks supports various file formats such as CSV, PARQUET, JSON, and more.\n",
    "\n",
    "    - The `FORMAT_OPTIONS` specifies file format options like reading the header and schema inference.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "**REQUIRED** - In the cell below add the path to YOUR **backup** volume in the `FROM` clause of `COPY INTO`. \n",
    "\n",
    "Example: `FROM '/Volumes/dbacademy/labuser1234_5678/backup'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5730d67-ca8c-48cd-bef2-b6dfa4b8b641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS au_products_copy_into;\n",
    "\n",
    "\n",
    "-- Create an empty table\n",
    "-- You can define the schema of the table if you desire\n",
    "CREATE TABLE au_products_copy_into(\n",
    "  productid STRING,\n",
    "  productname STRING,\n",
    "  listprice DOUBLE\n",
    ");\n",
    "\n",
    "\n",
    "-- Copy the files into the table and merge the schema\n",
    "COPY INTO au_products_copy_into\n",
    "FROM 'REPLACE WITH THE PATH TO YOUR backup VOLUME'     -- TO DO: Add your path to the backup volume here\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header'='true', 'inferSchema'='true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2fd18e2-dc4f-4be8-941d-acb454661483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Rerun the `COPY INTO` statement again after adding the path to your **backup** volume in the `FROM` clause again. \n",
    "\n",
    "    Notice that **num_affected_rows** and **num_inserted_rows** are both 0. Since all of the data was already read, `COPY INTO` does not ingest the file(s) again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5b1ee6a-4f4c-4448-8220-b69803fb0485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "COPY INTO au_products_copy_into\n",
    "FROM 'REPLACE WITH THE PATH TO YOUR backup VOLUME'   -- TO DO: Add your path to the backup volume here\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header'='true', 'inferSchema'='true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4015e42-41ba-4e71-81f4-fbb675714764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Display the **au_products_copy_into** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "674fbee8-33f3-4811-a2f3-0b00ae40f731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM au_products_copy_into;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "966c17c1-6228-4d9f-9bb4-2b64de99d78b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Summary: COPY INTO (legacy)\n",
    "The CREATE STREAMING TABLE SQL command is the recommended alternative to the legacy COPY INTO SQL command for incremental ingestion from cloud object storage. See COPY INTO. For a more scalable and robust file ingestion experience, Databricks recommends that SQL users leverage streaming tables instead of COPY INTO.\n",
    "\n",
    "[COPY INTO (legacy)](https://docs.databricks.com/aws/en/ingestion/#copy-into-legacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca2d9659-7075-4b18-a68e-d93ef53cd5e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C3. Introduction to Streaming Tables in DBSQL (Bonus)\n",
    "\n",
    "This is a high-level introduction to streaming tables in DBSQL. Streaming tables enable streaming or incremental data processing. Depending on your final objective, streaming tables can be extremely useful. \n",
    "\n",
    "\n",
    "  We will briefly cover the topic to familiarize you with its capabilities. For more details, check out the [CREATE STREAMING TABLE documentation](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-create-streaming-table). Databricks offers several features for managing streaming and real-time data ingestion, so remember to consult with your Data Engineering team for additional support with streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50431d0d-dbd5-487c-9ce9-d5b0550084b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. In this example, we will create a silver streaming table from a simple bronze table. In many scenarios, you will be reading from cloud storage to start the process. If you want to stream raw data into a table from cloud storage, view examples in the [CREATE STREAMING TABLE documentation](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-create-streaming-table).\n",
    "\n",
    "    To begin, let's create the table **emp_bronze_raw** with a list of employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4177be4d-d8f0-46a7-a25d-f3d8e089648d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Use our catalog and schema\n",
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA IDENTIFIER(DA.schema_name);\n",
    "\n",
    "-- Drop the tables if they exist to start from the beginning\n",
    "DROP TABLE IF EXISTS emp_bronze_raw;\n",
    "DROP TABLE IF EXISTS emp_silver_streaming;\n",
    "\n",
    "-- Create the employees table\n",
    "CREATE TABLE emp_bronze_raw (\n",
    "    EmployeeID INT,\n",
    "    FirstName VARCHAR(20),\n",
    "    Department VARCHAR(20)\n",
    ");\n",
    "\n",
    "-- Insert 5 rows of sample data\n",
    "INSERT INTO emp_bronze_raw (EmployeeID, FirstName, Department)\n",
    "VALUES\n",
    "(1, 'John', 'Marketing'),\n",
    "(2, 'Raul', 'HR'),\n",
    "(3, 'Michael', 'IT'),\n",
    "(4, 'Panagiotis', 'Finance'),\n",
    "(5, 'Aniket', 'Operations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dafbcfd8-bb62-42cd-8450-a8b641d8adb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Use the `CREATE OR REFRESH STREAMING TABLE` statement to create a streaming table named **emp_silver_streaming**. This table will incrementally load new rows as they are added to the **emp_bronze_raw** table. It will also create a new column named **IngestDateTime**, which records the date and time when the row was ingested.\n",
    "\n",
    "**NOTES:**\n",
    "- This process will take about a minute to run. Behind the scenes, streaming tables create a DLT pipeline. We will cover DLT in detail later in this course.\n",
    "- Streaming tables are supported only in DLT and on Databricks SQL with Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3c5afba-0310-4cf5-821d-9e7247efb9b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING TABLE emp_silver_streaming \n",
    "SCHEDULE EVERY 1 HOUR     -- Scheduling the refresh is optional\n",
    "SELECT \n",
    "  *, \n",
    "  current_timestamp() AS IngestDateTime\n",
    "FROM STREAM emp_bronze_raw;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c681dc7-7d7a-413c-8a49-fb5da0af3e34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. The `DESCRIBE HISTORY` statement displays a detailed list of all changes, versions, and metadata associated with a Delta table, including information on updates, deletions, and schema changes.\n",
    "\n",
    "    Run the cell below and view the results. Notice the following:\n",
    "\n",
    "    - In the **operation** column, you can see that a streaming table performs two operations: **DLT SETUP** and **STREAMING UPDATE**.\n",
    "\n",
    "    - Scroll to the right and find the **operationMetrics** column. In row 1 (Version 2 of the table), the value shows that the **numOutputRows** is 5, indicating that 5 rows were added to the **emp_silver_streaming** table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa44fee5-6e8f-4b33-9595-8c75f59ab1ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY emp_silver_streaming;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d7dece3-f0c0-4644-b2c9-d6d6051fb328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Run the cell below to query the **emp_silver_streaming** table. Notice that the results display 5 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b46877-2d8d-4bab-85f5-4a33dc7e4a4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM emp_silver_streaming;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c29902e4-ab8d-4767-85f7-28fa39e3e45d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Run the cell below to insert 2 rows of data into the originally **emp_bronze_raw** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6f38d78-824e-45c8-89eb-944aeec3c42e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INSERT INTO emp_bronze_raw (EmployeeID, FirstName, Department)\n",
    "VALUES\n",
    "(6, 'Athena', 'Marketing'),\n",
    "(7, 'Pedro', 'Training');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e9d4118-807b-4d11-bfae-adc4ca457369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. In our scenario, the scheduled refresh occurs every hour. However, we don't want to wait that long. Use the `REFRESH STREAMING TABLE` statement to refresh the streaming table **emp_silver_streaming**. This statement refreshes the data for a streaming table (or a materialized view, which will be covered later).\n",
    "\n",
    "\n",
    "    For more information to refresh a streaming table, view the [REFRESH (MATERIALIZED VIEW or STREAMING TABLE) documentation](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-refresh-full)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8a00504-f630-43a4-8b98-2b24601752b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "REFRESH STREAMING TABLE emp_silver_streaming;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84a79ebc-426e-4279-b16a-f61acf51cadd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Run the `DESCRIBE HISTORY` statement again to view the changes in the table.\n",
    "\n",
    "    Notice the following:\n",
    "    - The **emp_silver_streaming** table now has a new version, version 3.\n",
    "\n",
    "    - Scroll over to the **operationParameters** column. Notice that **outputMode** specifies an **Append** operation occurred.\n",
    "\n",
    "    - Scroll over to the **operationMetrics** column. Notice that the value of **numOutputRows** is 2, indicating an incremental update occurred and that two new rows were added to the **emp_silver_streaming** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83ec157e-2143-48df-ba6f-4d0bd81b956c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY emp_silver_streaming;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fcc74f9-ffbe-4a77-b154-0d70ebdc3a54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Run the query below to view the data in **emp_silver_streaming**. Notice that the table now contains 7 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "615350b5-4023-46dd-bce5-4eef8d1fdc7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM emp_silver_streaming;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f035d8b2-3b3a-4878-ad35-8275dc59b429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. Lastly, let's drop the two tables we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc2e9919-15a2-4c64-99b3-0dc967a8a441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS emp_bronze_raw;\n",
    "DROP TABLE IF EXISTS emp_silver_streaming;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cdc5a45-22c8-40cf-a180-12d26f5b54d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "2.2 Demo - Programmatic Exploration and Data Ingestion to Unity Catalog",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "SQL",
   "language": "sql",
   "name": "sql"
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
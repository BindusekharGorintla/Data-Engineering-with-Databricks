{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6e842d0-2898-4912-afe3-32061c6e831d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29fea5e0-8a57-4daa-a6e4-2ed9e0ef3970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3.1 - Deploying the Databricks Assets\n",
    "\n",
    "In Databricks, you have several options for deploying your Databricks Assets such as the UI, REST APIs, Databricks CLI, Databricks SDK or Databricks Asset Bundles (DABs). Databricks recommends Databricks Asset Bundles for creating, developing, deploying, and testing jobs and other Databricks resources as source code. \n",
    "\n",
    "In this demonstration, we will deploy our project and explore the job and pipeline using the Jobs and Pipelines UI. Then, we will examine the Workflow JSON and YAML structures and discuss how we can use these within our CI/CD process.\n",
    "\n",
    "## Objectives\n",
    "- Deploy Databricks assets using the Databricks SDK. \n",
    "- Analyze the Workflow JSON and YAML definitions for jobs and tasks, and explore their role in enabling automated deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "431ef16d-53d7-43bb-9190-8e72c8e6f2bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8383aa6a-527f-47d4-91a3-c9e6128e5ff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. \n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course.\n",
    "\n",
    "##### The notebook \"2.1 - Modularizing PySpark Code - Required\" sets up the catalogs for this course. If you have not run this notebook, the catalogs will not be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f963060b-6f86-46e8-8a1a-7541c9d23806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94bbae3f-8cc5-4358-8411-2466fefe8774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Create the Databricks Job with Notebooks, Python Files and a Lakeflow Declarative Spark Pipeline\n",
    "\n",
    "1. In this section, we will create the job for our project. The job will contain the following tasks:\n",
    "    - Unit tests\n",
    "    - Spark Declarative Pipeline with the ETL pipeline and integration tests \n",
    "    - Final data visualization deliverable for this project\n",
    "\n",
    "**FINAL JOB**\n",
    "\n",
    "![Final SDK Workflow](../Includes/images/05_final_sdk_workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a335b9d-d174-481f-a9b2-0666b440b2cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Confirm the pipeline from the previous demonstration exists. If not, create the Lakeflow Spark Declarative Pipeline and store the ID\n",
    "my_pipeline_id = obtain_pipeline_id_or_create_if_not_exists()\n",
    "print(my_pipeline_id)\n",
    "\n",
    "# ## Create the job\n",
    "create_demo_5_job(my_pipeline_id = my_pipeline_id, job_name = f'Dev Workflow Using the SDK_{DA.catalog_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dd1cc4f-c2f5-4cf7-a21f-85f5e338bbaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Complete the following steps to view and run the new job:\n",
    "\n",
    "    a. In the far-left navigation bar, right-click on **Jobs and Pipelines** and select *Open Link in New Tab*.\n",
    "\n",
    "    b. In the **Jobs & Pipelines** tab, you should see a job named **Dev Workflow Using the SDK_user_name**.\n",
    "\n",
    "    c. Select the job **Dev Workflow Using the SDK_user_name**.\n",
    "\n",
    "    d. Select on **Run now** to run the job.\n",
    "\n",
    "    e. Leave the job open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d39b3df-55c0-49e0-9666-960edcf8a004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "4. While the job is running, explore the job tasks. The job will take between 5-7 minutes to complete. Navigate to the job **Runs** tab. Here you should see the job executing.\n",
    "\n",
    "**Job Task Descriptions**\n",
    "![Final SDK Workflow Desc](../Includes/images/05_Final_Workflow_Desc.png)\n",
    "\n",
    "#####4a. Task 1: Unit_Tests\n",
    "\n",
    "- a. On the job **Runs** tab, right click on the square for **Unit_Tests** and select *Open Link in New Tab*.\n",
    "\n",
    "- b. Notice the **Run Unit Tasks** notebook executes the unit tests we created earlier.\n",
    "\n",
    "- c. Close the tab.\n",
    "\n",
    "\n",
    "#####4b. Task 2: Health_ETL\n",
    "\n",
    "- a. Select the **Tasks** tab and select the **Health_ETL** task.\n",
    "\n",
    "- b. In the **Task** section find the **Pipeline** value and select the icon to the right of the pipeline name to open the pipeline.\n",
    "\n",
    "   **NOTE:** If the Lakeflow Spark Declarative Pipeline has already completed, simply select the pipeline link.\n",
    "\n",
    "- c. Notice the pipeline executes (will execute) the ETL pipeline we created earlier.\n",
    "\n",
    "- d. Close the Spark Declarative Pipeline.\n",
    "\n",
    "\n",
    "#####4c. Task 3: Visualization\n",
    "\n",
    "- a. Select the job **Runs** tab, right click on the square for **Visualization** and select *Open Link in New Tab*.\n",
    "\n",
    "- b. Notice the **Final Visualization** notebook creates the final visualization for the project.\n",
    "\n",
    "- c. Close the tab.\n",
    "\n",
    "\n",
    "Leave the job open while it continues to run and continue the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0788f122-a5bf-4790-a0b0-1d4f1a87610e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Complete the following to view the JSON file to deploy this job:\n",
    "\n",
    "   a. Navigate back to the main Workflow job by selecting the **Tasks** tab.\n",
    "\n",
    "   b. At the top right of the job, select the kebab menu (three ellipsis icon near the **Run now** button).\n",
    "\n",
    "   c. Select **View as code**, then select **JSON** from the above tab.\n",
    "\n",
    "   d. Notice that you can view the JSON definition for the job for use with the REST API. This is a great way to easily obtain the necessary values to begin automating your deployment for the REST API (the SDK values are similar).\n",
    "\n",
    "   e. Close the **Job JSON** popup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ab5009a-4b8c-4506-b657-a8d1e35d4ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Complete the following to view the YAML file to deploy this job:\n",
    "\n",
    "   a. Confirm you are on the **Tasks** tab.\n",
    "\n",
    "   b. At the top right of the job, select the kebab menu (three ellipsis icon near the **Run now** button).\n",
    "\n",
    "   c. Select **Edit as YAML**.\n",
    "\n",
    "   d. Notice that you can view job as the YAML for the job. This is a great way to easily obtain the necessary values for the YAML deployment (this YAML file is extremely helpful when deploying using **Databricks Asset Bundles**).\n",
    "\n",
    "   e. In the top right select **Close editor**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "579ce8da-2689-4199-8969-34059b3a8612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. The job should be completed by now. View the completed job and confirm the three tasks completed successfully. Feel free to view the completed tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dccad28a-66f1-4094-b68a-f8bc2be42abe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Complete the following steps to view the Databricks SDK code to create the Workflow.\n",
    "\n",
    "    a. SDK Code: **[../Includes/Classroom-Setup-3.1]($../Includes/Classroom-Setup-3.1)**\n",
    "\n",
    "    b. Scroll down to cell 4: `def create_demo_5_job(my_pipeline_id, job_name)` \n",
    "    \n",
    "    c. Notice the amount of Python code used to create the Job to deploy our development code. \n",
    "    \n",
    "**NOTE:** Details of the Databricks SDK code is beyond the scope of this course.\n",
    "\n",
    "While the SDK provides low-level control over your deployment, it also requires significant time and effort to write all the necessary code.  \n",
    "\n",
    "In this example, we are only deploying the development job. Additional modifications will be needed to deploy both the staging and production jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e8df81a-d31a-4375-aa4d-59edcc695658",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next Steps for CI/CD\n",
    "\n",
    "Think about the following for the deploying this project utilizing the entire CI/CD process:\n",
    "- How will I automatically deploy the databricks assets to run for **dev**, **stage**, and **prod** environments?\n",
    "- How do I parameterize all the values I need based on the target environment?\n",
    "- How do I configure the necessary variables for the Lakeflow Spark Declarative Pipeline during each deployment?\n",
    "- How do I maintain all of the code?\n",
    "- How do I automate this entire process?\n",
    "- How can I setup continuous integration and continuous delivery or deployment (CI/CD) system, such as GitHub Actions, to automatically run your unit tests whenever your code changes? For an example, see the coverage of GitHub Actions in [Software engineering best practices for notebooks](https://docs.databricks.com/en/notebooks/best-practices.html). \n",
    "\n",
    "### Next Steps: \n",
    "\n",
    "In this course, you explored the essentials of CI/CD with a focus on continuous integration (CI) in Databricks. Next, youâ€™ll want to dive into the other half of the DevOps pipeline, continuous deployment (CD) by learning how to deploy assets using **Databricks Asset Bundles (DABs)**.\n",
    "\n",
    "#### DABs Documentation [What are Databricks Asset Bundles?](https://docs.databricks.com/en/dev-tools/bundles/index.html) \n",
    "\n",
    "#### Databricks Academy Course: [Automated Deployment with Databricks Asset Bundles](https://www.databricks.com/training/catalog/automated-deployment-with-databricks-asset-bundles-3724)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd4a1058-eba5-431e-a7ae-c53f4782d9c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "3.1 - Deploying the Databricks Assets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
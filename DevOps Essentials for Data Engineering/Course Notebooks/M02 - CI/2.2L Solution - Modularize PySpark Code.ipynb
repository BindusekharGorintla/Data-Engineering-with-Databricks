{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e865d2e-ac76-47c3-bb37-4d8857bd129f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef61b44c-f2fc-4cda-8bc5-5d1bfcf52ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.2 Lab - Modularize PySpark Code\n",
    "\n",
    "### Estimated Duration: 15-20 minutes\n",
    "\n",
    "By the end of this lab, you will practice analyzing a PySpark script by breaking it down into smaller, reusable functions, and assessing how well their changes improve the code's clarity and ease of maintenance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb38e3cd-626a-4559-9b5a-3413b0bfbac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e2402ff-9045-4ca5-a0ec-f917f8619839",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. \n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course.\n",
    "\n",
    "##### The notebook \"2.1 - Modularizing PySpark Code - Required\" sets up the catalogs for this course. If you have not run this notebook, the catalogs will not be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dbfbd81-6ac1-4cfd-a1ec-8dd9adf3f110",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-2.2L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a28d306-83d4-4ae5-a38a-ee6f40a300f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to view your current default catalog and schema. \n",
    "\n",
    "  Confirm the following:\n",
    "- The default catalog is your unique catalog name (shown above).\n",
    "- The current schema is **default**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "973946da-f308-4e5a-b5c0-f9b043fab7b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06ae4342-f044-41e2-85fd-fef80b524b2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Review the Provided PySpark Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "813857af-21ca-4d97-a0fa-5362fa14f767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to preview the **samples.nyctaxi.trips** table. Confirm the table exists and view the data.\n",
    "\n",
    "    Notice the following:\n",
    "    - All columns are in lower case\n",
    "    - **trip_distance** is currently in miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a79ef409-1b0a-4fca-a319-42ad52ed57a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM samples.nyctaxi.trips \n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e2bf4cc-f57c-439f-96a2-12a92a078392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. You have been provided with the following PySpark script that performs the following:\n",
    "\n",
    "   a. Reads from the **samples.nyctaxi.trips** table.\n",
    "\n",
    "   b. Creates a new column named **trip_distance_km** that converts **trip_distance** to kilometers and rounds it to two decimal places.\n",
    "\n",
    "   c. Converts all of the column names to uppercase.\n",
    "\n",
    "   d. Saves the DataFrame as a table named **nyc_lab_solution_table** in your specific catalog (`DA.catalog_name`).\n",
    "\n",
    "   Run the cell below and confirm that the **nyc_lab_solution_table** table was created with all uppercase column names and the new **trip_distance_km** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b214c9fe-1f0b-43a0-b024-d24faa645681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Run the code to view the default catalog the table is being written to.\n",
    "print(DA.catalog_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4880140-5d46-495b-bced-2f252783327c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load the data and create a new column named trip_distance_km\n",
    "new_taxi = (spark\n",
    "            .read\n",
    "            .table(\"samples.nyctaxi.trips\")\n",
    "            .withColumn(\"trip_distance_km\", F.round(F.col(\"trip_distance\") * 1.60934, 2))\n",
    "        )\n",
    "\n",
    "\n",
    "## Upper case all columns\n",
    "new_taxi = new_taxi.select([F.col(col).alias(col.upper()) for col in new_taxi.columns])\n",
    "\n",
    "\n",
    "## Save the table to the your catalog\n",
    "(new_taxi\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .saveAsTable(f'{DA.catalog_name}.default.nyc_lab_solution_table')\n",
    ")\n",
    "\n",
    "## View the final table\n",
    "display(spark.table(f'{DA.catalog_name}.default.nyc_lab_solution_table'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c95d8070-69f5-4140-9662-324986702baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Modularize the PySpark Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6396e0d6-7485-4112-90e4-94ffb2eda480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Your task is to take the provided Spark code from above and break it down into modular functions. Each function should perform a specific part of the task, making it easier to test, reuse, and maintain.\n",
    "\n",
    "    There are a variety of ways to solve this problem. For consistency in this example, create the following functions:\n",
    "\n",
    "    - `convert_miles_to_km`: Converts a column from miles to kilometers and rounds the result to two decimal places.\n",
    "\n",
    "    - `uppercase_column_names`: Converts all column names in the DataFrame to uppercase.\n",
    "    \n",
    "    - `load_data`: Reads the table.\n",
    "\n",
    "    - `save_to_catalog`: Saves the DataFrame as a new table in your catalog.\n",
    "\n",
    "**NOTE:** The `load_data` and `save_to_catalog` functions have already been created for you. \n",
    "\n",
    "**TO DO:** Create the `convert_miles_to_km` and `uppercase_column_names` in the cell below.\n",
    "\n",
    "**HINT:** The solution functions can be found in **[./src_lab/lab_functions/transforms.py]($./src_lab/lab_functions/transforms.py)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ba21594-26a7-4da7-bb66-d1f437c0defe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "## load_data has already been created for you\n",
    "def load_data(table_name):\n",
    "    return spark.read.table(table_name)\n",
    "\n",
    "\n",
    "## save_to_catalog has been created for you\n",
    "def save_to_catalog(df, catalog_name, schema_name, table_name):\n",
    "    (df\n",
    "     .write\n",
    "     .mode('overwrite')\n",
    "     .saveAsTable(f'{catalog_name}.{schema_name}.{table_name}')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57bafe9c-2507-458a-ba6f-5434d8e3b55e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## convert_miles_to_km\n",
    "def convert_miles_to_km(df, new_column_name, miles_column):\n",
    "    return df.withColumn(new_column_name, F.round(F.col(miles_column) * 1.60934, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48ae0035-674c-4c86-bbdc-4e0f79bccd60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## uppercase_column_names\n",
    "def uppercase_columns_names(df):\n",
    "    return df.select([F.col(col).alias(col.upper()) for col in df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c7fb80f-d7db-4bb6-98da-6c81d09c42d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run your functions to obtain the same results as the original PySpark code. The `save_to_catalog` function will name your new table **my_lab_table**. \n",
    "\n",
    "**NOTE:** If you are receiving a schema mismatch error that is because you are trying to overwrite a table you created with a different schema. Delete the table and recreate the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85a5dd91-2bdb-4b39-bf5e-e47ccb500273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Load table\n",
    "df = load_data(\"samples.nyctaxi.trips\")\n",
    "\n",
    "## Convert miles to km\n",
    "df = convert_miles_to_km(df, new_column_name = \"trip_distance_km\", miles_column = \"trip_distance\")\n",
    "\n",
    "## Upcase column\n",
    "df = uppercase_columns_names(df)\n",
    "\n",
    "## Save DataFrame as a table in your catalog\n",
    "save_to_catalog(df, catalog_name = DA.catalog_name, schema_name=\"default\", table_name = \"my_lab_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86c91800-3a78-486c-ba6f-8bec6e435c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Run the following cell to test that the original table created in cell 11 (**nyc_km_solution_table**) is the same as your new table created by the functions above (**my_lab_table**). The test uses the PySpark `assertDataFrameEqual` method.\n",
    "\n",
    "    If there is an error, it means the original table is not the same as your new table, and you need to fix your functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a5b67cf-659e-4306-923f-cdf9435d4d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "\n",
    "# Read the tables (solution and your created table)\n",
    "solution_df = spark.read.table(f\"{DA.catalog_name}.default.nyc_lab_solution_table\")\n",
    "user_df = spark.read.table(f\"{DA.catalog_name}.default.my_lab_table\")\n",
    "\n",
    "# Use assertDataFrameEqual to compare the two tables. Return an error if the tables are different.\n",
    "assertDataFrameEqual(solution_df, user_df)\n",
    "\n",
    "print(\"The tables are identical! Functions were created correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e038e1c-43d9-419f-9b66-c9f7a1d50096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "2.2L Solution - Modularize PySpark Code",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}